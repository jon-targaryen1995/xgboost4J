21/08/03 13:51:53 INFO StaticConf$: DB_HOME: /databricks
21/08/03 13:51:54 INFO DriverDaemon$: Current JVM Version 1.8.0_282
21/08/03 13:51:54 INFO DriverDaemon$: ========== driver starting up ==========
21/08/03 13:51:54 INFO DriverDaemon$: Java: Azul Systems, Inc. 1.8.0_282
21/08/03 13:51:54 INFO DriverDaemon$: OS: Linux/amd64 5.4.0-1051-azure
21/08/03 13:51:54 INFO DriverDaemon$: CWD: /databricks/driver
21/08/03 13:51:54 INFO DriverDaemon$: Mem: Max: 6.3G loaded GCs: PS Scavenge, PS MarkSweep
21/08/03 13:51:54 INFO DriverDaemon$: Logging multibyte characters: âœ“
21/08/03 13:51:54 INFO DriverDaemon$: 'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender
21/08/03 13:51:54 INFO DriverDaemon$: 'org.apache.log4j.Appender' appender in root logger: class com.codahale.metrics.log4j.InstrumentedAppender
21/08/03 13:51:54 INFO DriverDaemon$: 'null' appender in root logger: class com.databricks.logging.RequestTracker
21/08/03 13:51:54 INFO DriverDaemon$: == Modules:
21/08/03 13:51:55 INFO DriverDaemon$: Starting prometheus metrics log export timer
21/08/03 13:51:55 INFO DriverDaemon$: Universe Git Hash: 0e05dc66695549f02613b03dd51c896d45475f68
21/08/03 13:51:55 INFO DriverDaemon$: Spark Git Hash: f07e1840e22cfe6bac3e5a0502143b6c0abcdd47
21/08/03 13:51:55 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
21/08/03 13:51:55 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.,false,false,List())
21/08/03 13:51:55 INFO DatabricksILoop$: Creating throwaway interpreter
21/08/03 13:51:55 INFO MetastoreMonitor$: Internal internal metastore configured (config=DbMetastoreConfig{host=consolidated-westeuropec2-prod-metastore-0.mysql.database.azure.com, port=3306, dbName=organization6894037843183577, user=OzEIW9AZJo46MvrV@consolidated-westeuropec2-prod-metastore-0})
21/08/03 13:51:55 INFO DriverCorral: Creating the driver context
21/08/03 13:51:55 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-1601312997657594929-b655f8d9-2280-4e40-910e-d0e5f3a9b540
21/08/03 13:51:55 INFO HikariDataSource: metastore-monitor - Starting...
21/08/03 13:51:56 INFO HikariDataSource: metastore-monitor - Start completed.
21/08/03 13:51:56 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
21/08/03 13:51:56 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
21/08/03 13:51:56 INFO SparkContext: Running Spark version 3.1.0
21/08/03 13:51:56 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
21/08/03 13:51:56 INFO HikariDataSource: metastore-monitor - Shutdown completed.
21/08/03 13:51:56 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 919 milliseconds)
21/08/03 13:51:56 INFO ResourceUtils: ==============================================================
21/08/03 13:51:56 INFO ResourceUtils: No custom resources configured for spark.driver.
21/08/03 13:51:56 INFO ResourceUtils: ==============================================================
21/08/03 13:51:56 INFO SparkContext: Submitted application: Databricks Shell
21/08/03 13:51:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 5, script: , vendor: , memory -> name: memory, amount: 9216, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/08/03 13:51:56 INFO ResourceProfile: Limiting resource is cpus at 5 tasks per executor
21/08/03 13:51:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/08/03 13:51:57 INFO SecurityManager: Changing view acls to: root
21/08/03 13:51:57 INFO SecurityManager: Changing modify acls to: root
21/08/03 13:51:57 INFO SecurityManager: Changing view acls groups to: 
21/08/03 13:51:57 INFO SecurityManager: Changing modify acls groups to: 
21/08/03 13:51:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
21/08/03 13:51:57 INFO Utils: Successfully started service 'sparkDriver' on port 38103.
21/08/03 13:51:57 INFO SparkEnv: Registering MapOutputTracker
21/08/03 13:51:57 INFO SparkEnv: Registering BlockManagerMaster
21/08/03 13:51:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/08/03 13:51:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/08/03 13:51:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/08/03 13:51:57 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-775f6ddc-8c07-42f7-af13-f955778daaaf
21/08/03 13:51:57 INFO MemoryStore: MemoryStore started with capacity 3.3 GiB
21/08/03 13:51:57 INFO SparkEnv: Registering OutputCommitCoordinator
21/08/03 13:51:57 INFO SparkContext: Spark configuration:
eventLog.rolloverIntervalSeconds=3600
libraryDownload.sleepIntervalSeconds=5
libraryDownload.timeoutSeconds=180
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.app.startTime=1627998716248
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.acl.scim.client=com.databricks.spark.sql.acl.client.DriverToWebappScimClient
spark.databricks.cloudProvider=Azure
spark.databricks.cloudfetch.hasRegionSupport=true
spark.databricks.cloudfetch.requesterClassName=*********(redacted)
spark.databricks.clusterSource=UI
spark.databricks.clusterUsageTags.autoTerminationMinutes=0
spark.databricks.clusterUsageTags.azureSubscriptionId=25b08cf3-42f9-4444-b57c-127aef8d20f5
spark.databricks.clusterUsageTags.cloudProvider=Azure
spark.databricks.clusterUsageTags.clusterAllTags=*********(redacted)
spark.databricks.clusterUsageTags.clusterAvailability=ON_DEMAND_AZURE
spark.databricks.clusterUsageTags.clusterCreator=Webapp
spark.databricks.clusterUsageTags.clusterFirstOnDemand=1
spark.databricks.clusterUsageTags.clusterGeneration=67
spark.databricks.clusterUsageTags.clusterId=*********(redacted)
spark.databricks.clusterUsageTags.clusterLastActivityTime=1627986063890
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=true
spark.databricks.clusterUsageTags.clusterLogDestination=dbfs:/cluster-logs
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterName=RM_NL-Tuning
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=Standard_F16s_v2
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=6894037843183577
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=3
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=fixed_size
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterSpotBidMaxPrice=-1.0
spark.databricks.clusterUsageTags.clusterState=Restarting
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=5
spark.databricks.clusterUsageTags.clusterWorkers=5
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.dataPlaneRegion=westeurope
spark.databricks.clusterUsageTags.driverContainerId=24164f40774748c1b8f18cb4083463fd
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.48.11.69
spark.databricks.clusterUsageTags.driverInstanceId=29cfd8a82f0d48e495ce5b830f688926
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.48.11.5
spark.databricks.clusterUsageTags.driverNodeType=Standard_DS3_v2
spark.databricks.clusterUsageTags.driverPublicDns=137.116.198.74
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=true
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableLocalDiskEncryption=false
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.hailEnabled=false
spark.databricks.clusterUsageTags.instanceWorkerEnvId=workerenv-6894037843183577
spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType=vnet-injection
spark.databricks.clusterUsageTags.isIMv2Enabled=false
spark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)
spark.databricks.clusterUsageTags.managedResourceGroup=0005-d-nlarmn-ddxnlarmn
spark.databricks.clusterUsageTags.ngrokNpipEnabled=false
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=1
spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2=0
spark.databricks.clusterUsageTags.privateLinkEnabled=false
spark.databricks.clusterUsageTags.region=westeurope
spark.databricks.clusterUsageTags.sparkVersion=8.3.x-scala2.12
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-6894037843183577
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.delta.preview.enabled=true
spark.databricks.driverNfs.enabled=true
spark.databricks.driverNfs.pathSuffix=.ephemeral_nfs
spark.databricks.driverNodeTypeId=Standard_DS3_v2
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.managedCatalog.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.oauth.refresher.impl=*********(redacted)
spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class=com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.repl.enableClassFileCleanup=true
spark.databricks.secret.envVar.keys.toRedact=*********(redacted)
spark.databricks.secret.sparkConf.keys.toRedact=*********(redacted)
spark.databricks.service.dbutils.repl.backend=com.databricks.dbconnect.ReplDBUtils
spark.databricks.service.dbutils.server.backend=com.databricks.dbconnect.SparkServerDBUtils
spark.databricks.session.share=false
spark.databricks.sparkContextId=1601312997657594929
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.tahoe.logStore.gcp.class=com.databricks.tahoe.store.GCPLogStore
spark.databricks.workerNodeTypeId=Standard_F16s_v2
spark.databricks.workspace.matplotlibInline.enabled=true
spark.databricks.workspace.multipleResults.enabled=true
spark.driver.allowMultipleContexts=false
spark.driver.host=10.48.11.69
spark.driver.maxResultSize=4g
spark.driver.port=38103
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.cores=5
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Ddatabricks.serviceName=spark-executor-1
spark.executor.id=driver
spark.executor.memory=9g
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories=false
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.fs.AbstractFileSystem.gs.impl=shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS
spark.hadoop.fs.abfs.impl=shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=true
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.cpfs-abfss.impl=*********(redacted)
spark.hadoop.fs.cpfs-abfss.impl.disable.cache=true
spark.hadoop.fs.cpfs-adl.impl=*********(redacted)
spark.hadoop.fs.cpfs-adl.impl.disable.cache=true
spark.hadoop.fs.cpfs-s3.impl=*********(redacted)
spark.hadoop.fs.cpfs-s3a.impl=*********(redacted)
spark.hadoop.fs.cpfs-s3n.impl=*********(redacted)
spark.hadoop.fs.file.impl=com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem
spark.hadoop.fs.gs.impl=shaded.databricks.V2_1_4.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
spark.hadoop.fs.gs.impl.disable.cache=true
spark.hadoop.fs.gs.outputstream.upload.chunk.size=16777216
spark.hadoop.fs.mcfs-abfss.impl=com.databricks.sql.acl.fs.ManagedCatalogFileSystem
spark.hadoop.fs.mcfs-abfss.impl.disable.cache=true
spark.hadoop.fs.mcfs-s3.impl=com.databricks.sql.acl.fs.ManagedCatalogFileSystem
spark.hadoop.fs.mcfs-s3a.impl=com.databricks.sql.acl.fs.ManagedCatalogFileSystem
spark.hadoop.fs.mcfs-s3n.impl=com.databricks.sql.acl.fs.ManagedCatalogFileSystem
spark.hadoop.fs.s3.impl=shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.active.blocks=32
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.enable.doAs=false
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled=false
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.parquet.block.size.row.check.max=10
spark.hadoop.parquet.block.size.row.check.min=10
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.parquet.page.size.check.estimate=false
spark.hadoop.parquet.page.verify-checksum.enabled=true
spark.hadoop.parquet.page.write-checksum.enabled=true
spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled=true
spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException=true
spark.hadoop.spark.driverproxy.customHeadersToProperties=*********(redacted)
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.48.11.69:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-1601312997657594929-b655f8d9-2280-4e40-910e-d0e5f3a9b540
spark.rpc.message.maxSize=256
spark.scheduler.listenerbus.eventqueue.capacity=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=/databricks/hive/*
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=0.13.0
spark.sql.legacy.createHiveTableByDefault=false
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.sources.default=delta
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.streaming.stopTimeout=15s
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=47313
spark.worker.cleanup.enabled=false
21/08/03 13:51:57 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
21/08/03 13:51:58 INFO log: Logging initialized @6289ms to org.eclipse.jetty.util.log.Slf4jLog
21/08/03 13:51:58 INFO Server: jetty-9.4.36.v20210114; built: 2021-01-14T16:44:28.689Z; git: 238ec6997c7806b055319a6d11f8ae7564adc0de; jvm 1.8.0_282-b08
21/08/03 13:51:58 INFO Server: Started @6423ms
21/08/03 13:51:58 INFO AbstractConnector: Started ServerConnector@71560f51{HTTP/1.1, (http/1.1)}{10.48.11.69:47313}
21/08/03 13:51:58 INFO Utils: Successfully started service 'SparkUI' on port 47313.
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@617449dd{/jobs,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f054f70{/jobs/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@27b337bb{/jobs/job,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6cbb175{/jobs/job/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7ab802f4{/stages,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3b97907c{/stages/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@631c6d11{/stages/stage,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@70997a94{/stages/stage/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6c538eb2{/stages/pool,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@487cd177{/stages/pool/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@421d54b3{/storage,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4dad8ec0{/storage/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@ccd341d{/storage/rdd,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6ee5f485{/storage/rdd/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@45592af7{/environment,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@36359723{/environment/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@16b3c905{/executors,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@53eba4b8{/executors/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b88ca8e{/executors/threadDump,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@61608e1a{/executors/threadDump/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7197b07f{/executors/heapHistogram,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7a65a360{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7cff3f1d{/static,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1a21f43f{/,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@241fbec{/api,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4a70d302{/jobs/job/kill,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@622d7e4{/stages/stage/kill,null,AVAILABLE,@Spark}
21/08/03 13:51:58 INFO SparkUI: Bound SparkUI to 10.48.11.69, and started at http://10.48.11.69:47313
21/08/03 13:51:58 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
21/08/03 13:51:58 INFO FairSchedulableBuilder: Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
21/08/03 13:51:58 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.48.11.69:7077...
21/08/03 13:51:58 INFO TransportClientFactory: Successfully created connection to /10.48.11.69:7077 after 76 ms (0 ms spent in bootstraps)
21/08/03 13:51:59 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20210803135158-0000
21/08/03 13:51:59 INFO TaskSchedulerImpl: Task preemption enabled.
21/08/03 13:51:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43057.
21/08/03 13:51:59 INFO NettyBlockTransferService: Server created on 10.48.11.69:43057
21/08/03 13:51:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/08/03 13:51:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.48.11.69, 43057, None)
21/08/03 13:51:59 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.69:43057 with 3.3 GiB RAM, BlockManagerId(driver, 10.48.11.69, 43057, None)
21/08/03 13:51:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.48.11.69, 43057, None)
21/08/03 13:51:59 INFO BlockManager: external shuffle service port = 4048
21/08/03 13:51:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.48.11.69, 43057, None)
21/08/03 13:51:59 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7e5efcab{/metrics/json,null,AVAILABLE,@Spark}
21/08/03 13:51:59 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener
21/08/03 13:51:59 INFO DBCEventLoggingListener: Logging events to eventlogs/1601312997657594929/eventlog
21/08/03 13:51:59 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
21/08/03 13:51:59 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
21/08/03 13:51:59 INFO SparkContext: Loading Spark Service RPC Server
21/08/03 13:51:59 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/0 on worker-20210803135158-10.48.11.72-41159 (10.48.11.72:41159) with 5 core(s)
21/08/03 13:51:59 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/0 on hostPort 10.48.11.72:41159 with 5 core(s), 9.0 GiB RAM
21/08/03 13:51:59 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/1 on worker-20210803135158-10.48.11.72-41159 (10.48.11.72:41159) with 5 core(s)
21/08/03 13:51:59 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/1 on hostPort 10.48.11.72:41159 with 5 core(s), 9.0 GiB RAM
21/08/03 13:51:59 INFO SparkServiceRPCServer: Starting Spark Service RPC Server
21/08/03 13:51:59 INFO Server: jetty-9.4.36.v20210114; built: 2021-01-14T16:44:28.689Z; git: 238ec6997c7806b055319a6d11f8ae7564adc0de; jvm 1.8.0_282-b08
21/08/03 13:51:59 INFO AbstractConnector: Started ServerConnector@38159384{HTTP/1.1, (http/1.1)}{0.0.0.0:15001}
21/08/03 13:51:59 INFO Server: Started @7824ms
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/1 is now RUNNING
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/0 is now RUNNING
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/2 on worker-20210803135159-10.48.11.71-36425 (10.48.11.71:36425) with 5 core(s)
21/08/03 13:52:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/2 on hostPort 10.48.11.71:36425 with 5 core(s), 9.0 GiB RAM
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/3 on worker-20210803135159-10.48.11.71-36425 (10.48.11.71:36425) with 5 core(s)
21/08/03 13:52:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/3 on hostPort 10.48.11.71:36425 with 5 core(s), 9.0 GiB RAM
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/4 on worker-20210803135159-10.48.11.74-40475 (10.48.11.74:40475) with 5 core(s)
21/08/03 13:52:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/4 on hostPort 10.48.11.74:40475 with 5 core(s), 9.0 GiB RAM
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/5 on worker-20210803135159-10.48.11.74-40475 (10.48.11.74:40475) with 5 core(s)
21/08/03 13:52:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/5 on hostPort 10.48.11.74:40475 with 5 core(s), 9.0 GiB RAM
21/08/03 13:52:00 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
21/08/03 13:52:00 INFO DatabricksILoop$: Successfully initialized SparkContext
21/08/03 13:52:00 INFO SharedState: Scheduler stats enabled.
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/5 is now RUNNING
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/4 is now RUNNING
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/2 is now RUNNING
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/3 is now RUNNING
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/6 on worker-20210803135159-10.48.11.70-46581 (10.48.11.70:46581) with 5 core(s)
21/08/03 13:52:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/6 on hostPort 10.48.11.70:46581 with 5 core(s), 9.0 GiB RAM
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/7 on worker-20210803135159-10.48.11.70-46581 (10.48.11.70:46581) with 5 core(s)
21/08/03 13:52:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/7 on hostPort 10.48.11.70:46581 with 5 core(s), 9.0 GiB RAM
21/08/03 13:52:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
21/08/03 13:52:00 INFO SharedState: Warehouse path is '/user/hive/warehouse'.
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/7 is now RUNNING
21/08/03 13:52:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/6 is now RUNNING
21/08/03 13:52:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1cee2e10{/SQL,null,AVAILABLE,@Spark}
21/08/03 13:52:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@33de7f3d{/SQL/json,null,AVAILABLE,@Spark}
21/08/03 13:52:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@c5e69a5{/SQL/execution,null,AVAILABLE,@Spark}
21/08/03 13:52:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@11e17893{/SQL/execution/json,null,AVAILABLE,@Spark}
21/08/03 13:52:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@d84b3a2{/static/sql,null,AVAILABLE,@Spark}
21/08/03 13:52:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@448cdb47{/storage/iocache,null,AVAILABLE,@Spark}
21/08/03 13:52:00 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2b01c689{/storage/iocache/json,null,AVAILABLE,@Spark}
21/08/03 13:52:01 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:52:01 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:52:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/8 on worker-20210803135200-10.48.11.68-38115 (10.48.11.68:38115) with 5 core(s)
21/08/03 13:52:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/8 on hostPort 10.48.11.68:38115 with 5 core(s), 9.0 GiB RAM
21/08/03 13:52:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20210803135158-0000/9 on worker-20210803135200-10.48.11.68-38115 (10.48.11.68:38115) with 5 core(s)
21/08/03 13:52:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20210803135158-0000/9 on hostPort 10.48.11.68:38115 with 5 core(s), 9.0 GiB RAM
21/08/03 13:52:01 INFO DatabricksILoop$: Finished creating throwaway interpreter
21/08/03 13:52:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/9 is now RUNNING
21/08/03 13:52:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20210803135158-0000/8 is now RUNNING
21/08/03 13:52:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.72:35302) with ID 0,  ResourceProfileId 0
21/08/03 13:52:02 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.72:35304) with ID 1,  ResourceProfileId 0
21/08/03 13:52:02 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.72:43787 with 4.6 GiB RAM, BlockManagerId(0, 10.48.11.72, 43787, None)
21/08/03 13:52:02 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.72:45133 with 4.6 GiB RAM, BlockManagerId(1, 10.48.11.72, 45133, None)
21/08/03 13:52:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.71:33644) with ID 2,  ResourceProfileId 0
21/08/03 13:52:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.74:48826) with ID 4,  ResourceProfileId 0
21/08/03 13:52:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.71:33646) with ID 3,  ResourceProfileId 0
21/08/03 13:52:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.74:48828) with ID 5,  ResourceProfileId 0
21/08/03 13:52:03 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.74:38715 with 4.6 GiB RAM, BlockManagerId(4, 10.48.11.74, 38715, None)
21/08/03 13:52:03 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.71:45657 with 4.6 GiB RAM, BlockManagerId(2, 10.48.11.71, 45657, None)
21/08/03 13:52:03 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.71:46151 with 4.6 GiB RAM, BlockManagerId(3, 10.48.11.71, 46151, None)
21/08/03 13:52:03 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.74:35137 with 4.6 GiB RAM, BlockManagerId(5, 10.48.11.74, 35137, None)
21/08/03 13:52:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.70:51296) with ID 7,  ResourceProfileId 0
21/08/03 13:52:03 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.70:51298) with ID 6,  ResourceProfileId 0
21/08/03 13:52:03 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.70:35939 with 4.6 GiB RAM, BlockManagerId(7, 10.48.11.70, 35939, None)
21/08/03 13:52:03 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.70:38821 with 4.6 GiB RAM, BlockManagerId(6, 10.48.11.70, 38821, None)
21/08/03 13:52:04 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.68:60340) with ID 8,  ResourceProfileId 0
21/08/03 13:52:04 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.48.11.68:60342) with ID 9,  ResourceProfileId 0
21/08/03 13:52:04 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.68:32815 with 4.6 GiB RAM, BlockManagerId(8, 10.48.11.68, 32815, None)
21/08/03 13:52:04 INFO DatabricksMountsStore: Mount store initialization: Attempting to get the list of mounts from metadata manager of DBFS
21/08/03 13:52:04 INFO BlockManagerMasterEndpoint: Registering block manager 10.48.11.68:44227 with 4.6 GiB RAM, BlockManagerId(9, 10.48.11.68, 44227, None)
21/08/03 13:52:04 INFO log: Logging initialized @12772ms to shaded.v9_4.org.eclipse.jetty.util.log.Slf4jLog
21/08/03 13:52:04 INFO TypeUtil: JVM Runtime does not support Modules
21/08/03 13:52:05 INFO DatabricksMountsStore: Mount store initialization: Received a list of 13 mounts accessible from metadata manager of DBFS
21/08/03 13:52:05 INFO DatabricksMountsStore: Updated mounts cache. Changes: List((+,DbfsMountPoint(s3a://databricks-datasets-oregon/, /databricks-datasets)), (+,DbfsMountPoint(abfss://ddlasnlabesrm.dfs.core.windows.net/, /mnt/ddlasnlabesrm_dldata)), (+,DbfsMountPoint(abfss://ddlasnlanlsrm.dfs.core.windows.net/, /mnt/ddlasnlanlsrm_dldata)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-tracking)), (+,DbfsMountPoint(abfss://dstgnlarmn.dfs.core.windows.net/, /mnt/newMountPoint)), (+,DbfsMountPoint(wasbs://dbstoragesy3bchcqjgny4.blob.core.windows.net/6894037843183577, /databricks-results)), (+,DbfsMountPoint(abfss://dstgnlarmn.dfs.core.windows.net/, /mnt/dstgnlarmn/be-rm)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-registry)), (+,DbfsMountPoint(abfss://dstgnlarmn.dfs.core.windows.net/, /mnt/dstgnlarmn/ddxnlarmn)), (+,DbfsMountPoint(abfss://dstgnlarmn.dfs.core.windows.net/, /mnt/dstgnlarmn_nl-rm)), (+,DbfsMountPoint(abfss://dstgnlarmn.dfs.core.windows.net/, /mnt/dstgnlarmn_ddxnlarmn)), (+,DbfsMountPoint(abfss://dstgnlarmn.dfs.core.windows.net/, /mnt/dstgnlarmn_be-rm)), (+,DbfsMountPoint(wasbs://dbstoragesy3bchcqjgny4.blob.core.windows.net/6894037843183577, /)))
21/08/03 13:52:05 INFO DatabricksFileSystemV2Factory: Creating wasbs file system for wasbs://root@dbstoragesy3bchcqjgny4.blob.core.windows.net
21/08/03 13:52:05 INFO NativeAzureFileSystem: Delete with limit configurations: deleteFileCountLimitEnabled=false, deleteFileCountLimit=-1
21/08/03 13:52:05 INFO DBFS: Initialized DBFS with DBFSV2 as the delegate.
21/08/03 13:52:05 INFO HiveConf: Found configuration file file:/databricks/hive/conf/hive-site.xml
21/08/03 13:52:05 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
21/08/03 13:52:05 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
21/08/03 13:52:05 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
21/08/03 13:52:05 INFO AbstractService: Service:OperationManager is inited.
21/08/03 13:52:05 INFO AbstractService: Service:SessionManager is inited.
21/08/03 13:52:05 INFO AbstractService: Service: CLIService is inited.
21/08/03 13:52:05 INFO AbstractService: Service:ThriftHttpCLIService is inited.
21/08/03 13:52:05 INFO AbstractService: Service: HiveServer2 is inited.
21/08/03 13:52:05 INFO AbstractService: Service:OperationManager is started.
21/08/03 13:52:05 INFO AbstractService: Service:SessionManager is started.
21/08/03 13:52:05 INFO AbstractService: Service: CLIService is started.
21/08/03 13:52:05 INFO AbstractService: Service:ThriftHttpCLIService is started.
21/08/03 13:52:05 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
21/08/03 13:52:05 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
21/08/03 13:52:06 INFO Server: jetty-9.4.36.v20210114; built: 2021-01-14T16:44:28.689Z; git: 238ec6997c7806b055319a6d11f8ae7564adc0de; jvm 1.8.0_282-b08
21/08/03 13:52:06 INFO session: DefaultSessionIdManager workerName=node0
21/08/03 13:52:06 INFO session: No SessionScavenger set, using defaults
21/08/03 13:52:06 INFO session: node0 Scavenging every 600000ms
21/08/03 13:52:06 WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@4b7a4c83{/,null,STARTING} has uncovered http methods for path: /*
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b7a4c83{/,null,AVAILABLE}
21/08/03 13:52:06 INFO SslContextFactory: x509=X509@3b780b5c(1,h=[az-westeurope-c2.workers.prod.ns.databricks.com],w=[]) for Server@13dc383b[provider=null,keyStore=file:///databricks/keys/jetty-ssl-driver-keystore.jks,trustStore=null]
21/08/03 13:52:06 INFO AbstractConnector: Started ServerConnector@684f7f1c{SSL, (ssl, http/1.1)}{0.0.0.0:10000}
21/08/03 13:52:06 INFO Server: Started @14331ms
21/08/03 13:52:06 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
21/08/03 13:52:06 INFO AbstractService: Service:HiveServer2 is started.
21/08/03 13:52:06 INFO HiveThriftServer2: HiveThriftServer2 started
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@51aaa9d4{/sqlserver,null,AVAILABLE,@Spark}
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@65bb4cb9{/sqlserver/json,null,AVAILABLE,@Spark}
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@51c4992e{/sqlserver/session,null,AVAILABLE,@Spark}
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@198c0f1c{/sqlserver/session/json,null,AVAILABLE,@Spark}
21/08/03 13:52:06 WARN LibraryUtils$: Library file for validation /databricks/.python-env/packages_to_validate.json does not exist
21/08/03 13:52:06 INFO DriverCorral: Creating the driver context
21/08/03 13:52:06 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@79c2bc34{/StreamingQuery,null,AVAILABLE,@Spark}
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@15a8bbe5{/StreamingQuery/json,null,AVAILABLE,@Spark}
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@160e45c8{/StreamingQuery/statistics,null,AVAILABLE,@Spark}
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@17e8caf2{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}
21/08/03 13:52:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@72a61e61{/static/sql,null,AVAILABLE,@Spark}
21/08/03 13:52:06 INFO DriverDaemon: Starting driver daemon...
21/08/03 13:52:06 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
21/08/03 13:52:06 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
21/08/03 13:52:06 INFO DriverDaemon$: Attempting to run: 'set up ttyd daemon'
21/08/03 13:52:06 INFO DriverDaemon$: Attempting to run: 'Configuring RStudio daemon'
21/08/03 13:52:06 INFO Server: jetty-9.4.36.v20210114; built: 2021-01-14T16:44:28.689Z; git: 238ec6997c7806b055319a6d11f8ae7564adc0de; jvm 1.8.0_282-b08
21/08/03 13:52:06 INFO DriverDaemon$$anon$1: Message out thread ready
21/08/03 13:52:06 INFO AbstractConnector: Started ServerConnector@7abd75fc{HTTP/1.1, (http/1.1)}{0.0.0.0:6061}
21/08/03 13:52:06 INFO Server: Started @14805ms
21/08/03 13:52:06 INFO DriverDaemon: Driver daemon started.
21/08/03 13:52:08 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:52:08 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:52:08 INFO DriverCorral: Loading the root classloader
21/08/03 13:52:08 INFO DriverCorral: Starting sql repl ReplId-5a668-261a8-c36ff-f
21/08/03 13:52:08 INFO DriverCorral: Starting sql repl ReplId-6270b-a564a-6946b-c
21/08/03 13:52:08 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/08/03 13:52:08 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/08/03 13:52:08 INFO SQLDriverWrapper: setupRepl:ReplId-5a668-261a8-c36ff-f: finished to load
21/08/03 13:52:08 INFO SQLDriverWrapper: setupRepl:ReplId-6270b-a564a-6946b-c: finished to load
21/08/03 13:52:08 INFO DriverCorral: Starting sql repl ReplId-769ff-71735-7eeed
21/08/03 13:52:08 INFO DriverCorral: Starting sql repl ReplId-74d3b-bd71b-3ab98-2
21/08/03 13:52:08 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/08/03 13:52:08 INFO SQLDriverWrapper: setupRepl:ReplId-769ff-71735-7eeed: finished to load
21/08/03 13:52:08 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/08/03 13:52:08 INFO SQLDriverWrapper: setupRepl:ReplId-74d3b-bd71b-3ab98-2: finished to load
21/08/03 13:52:08 INFO DriverCorral: Starting sql repl ReplId-35ba0-ee20b-d9acb-e
21/08/03 13:52:08 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/08/03 13:52:08 INFO SQLDriverWrapper: setupRepl:ReplId-35ba0-ee20b-d9acb-e: finished to load
21/08/03 13:52:08 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:52:08 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:52:08 INFO DriverCorral: Starting r repl ReplId-506c9-2260b-72db4-3
21/08/03 13:52:08 WARN RDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/08/03 13:52:08 INFO ROutputStreamHandler: Connection succeeded on port 45885
21/08/03 13:52:08 INFO ROutputStreamHandler: Connection succeeded on port 35721
21/08/03 13:52:08 INFO RDriverLocal: 1. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: object created with for ReplId-506c9-2260b-72db4-3.
21/08/03 13:52:08 INFO RDriverLocal: 2. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: initializing ...
21/08/03 13:52:08 INFO RDriverLocal: 3. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: started RBackend thread on port 35443
21/08/03 13:52:08 INFO RDriverLocal: 4. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: waiting for SparkR to be installed ...
21/08/03 13:52:08 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/reflectasm-1.11.3.jar,,NONE))
21/08/03 13:52:08 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/reflectasm-1.11.3.jar,,NONE))
21/08/03 13:52:08 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/reflectasm-1.11.3.jar,,NONE)
21/08/03 13:52:08 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/reflectasm-1.11.3.jar,,NONE)
21/08/03 13:52:08 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/reflectasm-1.11.3.jar,,NONE) is downloaded
21/08/03 13:52:09 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/reflectasm-1.11.3.jar,,NONE) as local file /local_disk0/tmp/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar in 697 milliseconds
21/08/03 13:52:09 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/reflectasm-1.11.3.jar,,NONE) to local file /local_disk0/tmp/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar 
21/08/03 13:52:09 INFO SparkContext: Added file /local_disk0/tmp/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar at spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar with timestamp 1627998729694
21/08/03 13:52:09 INFO Utils: Copying /local_disk0/tmp/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar
21/08/03 13:52:09 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar at spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar with timestamp 1627998729744
21/08/03 13:52:09 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/com/esotericsoftware/reflectasm-1.11.3.jar to Spark
21/08/03 13:52:09 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/com/esotericsoftware/reflectasm-1.11.3.jar
21/08/03 13:52:09 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/org/objenesis/objenesis-2.5.1.jar,,NONE))
21/08/03 13:52:09 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/org/objenesis/objenesis-2.5.1.jar,,NONE))
21/08/03 13:52:09 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/org/objenesis/objenesis-2.5.1.jar,,NONE)
21/08/03 13:52:09 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/org/objenesis/objenesis-2.5.1.jar,,NONE)
21/08/03 13:52:09 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/org/objenesis/objenesis-2.5.1.jar,,NONE) is downloaded
21/08/03 13:52:09 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/org/objenesis/objenesis-2.5.1.jar,,NONE) as local file /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar in 45 milliseconds
21/08/03 13:52:09 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/org/objenesis/objenesis-2.5.1.jar,,NONE) to local file /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar 
21/08/03 13:52:09 INFO SparkContext: Added file /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar at spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar with timestamp 1627998729829
21/08/03 13:52:09 INFO Utils: Copying /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile8305477370858712955objenesis_2_5_1-7628f.jar
21/08/03 13:52:09 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar at spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar with timestamp 1627998729872
21/08/03 13:52:09 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/org/objenesis/objenesis-2.5.1.jar to Spark
21/08/03 13:52:09 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/org/objenesis/objenesis-2.5.1.jar
21/08/03 13:52:09 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.3.jar,,NONE))
21/08/03 13:52:09 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.3.jar,,NONE))
21/08/03 13:52:09 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.3.jar,,NONE)
21/08/03 13:52:09 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.3.jar,,NONE)
21/08/03 13:52:09 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.3.jar,,NONE) is downloaded
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.3.jar,,NONE) as local file /local_disk0/tmp/addedFile1775982106135659717config_1_3_3-49c52.jar in 137 milliseconds
21/08/03 13:52:10 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.3.jar,,NONE) to local file /local_disk0/tmp/addedFile1775982106135659717config_1_3_3-49c52.jar 
21/08/03 13:52:10 INFO SparkContext: Added file /local_disk0/tmp/addedFile1775982106135659717config_1_3_3-49c52.jar at spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar with timestamp 1627998730036
21/08/03 13:52:10 INFO Utils: Copying /local_disk0/tmp/addedFile1775982106135659717config_1_3_3-49c52.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile1775982106135659717config_1_3_3-49c52.jar
21/08/03 13:52:10 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile1775982106135659717config_1_3_3-49c52.jar at spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar with timestamp 1627998730074
21/08/03 13:52:10 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.3.jar to Spark
21/08/03 13:52:10 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/com/typesafe/config-1.3.3.jar
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-xml_2.12-1.0.6.jar,,NONE))
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-xml_2.12-1.0.6.jar,,NONE))
21/08/03 13:52:10 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-xml_2.12-1.0.6.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-xml_2.12-1.0.6.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-xml_2.12-1.0.6.jar,,NONE) is downloaded
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-xml_2.12-1.0.6.jar,,NONE) as local file /local_disk0/tmp/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar in 115 milliseconds
21/08/03 13:52:10 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-xml_2.12-1.0.6.jar,,NONE) to local file /local_disk0/tmp/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar 
21/08/03 13:52:10 INFO SparkContext: Added file /local_disk0/tmp/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar at spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar with timestamp 1627998730219
21/08/03 13:52:10 INFO Utils: Copying /local_disk0/tmp/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar
21/08/03 13:52:10 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar at spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar with timestamp 1627998730232
21/08/03 13:52:10 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-xml_2.12-1.0.6.jar to Spark
21/08/03 13:52:10 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-xml_2.12-1.0.6.jar
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j_2.12-1.4.1.jar,,NONE))
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j_2.12-1.4.1.jar,,NONE))
21/08/03 13:52:10 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j_2.12-1.4.1.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j_2.12-1.4.1.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j_2.12-1.4.1.jar,,NONE) is downloaded
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j_2.12-1.4.1.jar,,NONE) as local file /local_disk0/tmp/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar in 270 milliseconds
21/08/03 13:52:10 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j_2.12-1.4.1.jar,,NONE) to local file /local_disk0/tmp/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar 
21/08/03 13:52:10 INFO SparkContext: Added file /local_disk0/tmp/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar at spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar with timestamp 1627998730522
21/08/03 13:52:10 INFO Utils: Copying /local_disk0/tmp/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar
21/08/03 13:52:10 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar at spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar with timestamp 1627998730537
21/08/03 13:52:10 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j_2.12-1.4.1.jar to Spark
21/08/03 13:52:10 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j_2.12-1.4.1.jar
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/akka/akka-actor_2.12-2.5.23.jar,,NONE))
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/akka/akka-actor_2.12-2.5.23.jar,,NONE))
21/08/03 13:52:10 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/akka/akka-actor_2.12-2.5.23.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/akka/akka-actor_2.12-2.5.23.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/akka/akka-actor_2.12-2.5.23.jar,,NONE) is downloaded
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/akka/akka-actor_2.12-2.5.23.jar,,NONE) as local file /local_disk0/tmp/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar in 155 milliseconds
21/08/03 13:52:10 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/com/typesafe/akka/akka-actor_2.12-2.5.23.jar,,NONE) to local file /local_disk0/tmp/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar 
21/08/03 13:52:10 INFO SparkContext: Added file /local_disk0/tmp/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar at spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar with timestamp 1627998730721
21/08/03 13:52:10 INFO Utils: Copying /local_disk0/tmp/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar
21/08/03 13:52:10 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar at spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar with timestamp 1627998730735
21/08/03 13:52:10 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/com/typesafe/akka/akka-actor_2.12-2.5.23.jar to Spark
21/08/03 13:52:10 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/com/typesafe/akka/akka-actor_2.12-2.5.23.jar
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/commons-logging/commons-logging-1.2.jar,,NONE))
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/commons-logging/commons-logging-1.2.jar,,NONE))
21/08/03 13:52:10 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/commons-logging/commons-logging-1.2.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/commons-logging/commons-logging-1.2.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/commons-logging/commons-logging-1.2.jar,,NONE) is downloaded
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/commons-logging/commons-logging-1.2.jar,,NONE) as local file /local_disk0/tmp/addedFile8241248691510676812commons_logging_1_2-0b642.jar in 28 milliseconds
21/08/03 13:52:10 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/commons-logging/commons-logging-1.2.jar,,NONE) to local file /local_disk0/tmp/addedFile8241248691510676812commons_logging_1_2-0b642.jar 
21/08/03 13:52:10 INFO SparkContext: Added file /local_disk0/tmp/addedFile8241248691510676812commons_logging_1_2-0b642.jar at spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar with timestamp 1627998730782
21/08/03 13:52:10 INFO Utils: Copying /local_disk0/tmp/addedFile8241248691510676812commons_logging_1_2-0b642.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile8241248691510676812commons_logging_1_2-0b642.jar
21/08/03 13:52:10 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8241248691510676812commons_logging_1_2-0b642.jar at spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar with timestamp 1627998730805
21/08/03 13:52:10 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/commons-logging/commons-logging-1.2.jar to Spark
21/08/03 13:52:10 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/commons-logging/commons-logging-1.2.jar
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/org/scalatest/scalatest_2.12-3.0.5.jar,,NONE))
21/08/03 13:52:10 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/org/scalatest/scalatest_2.12-3.0.5.jar,,NONE))
21/08/03 13:52:10 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/org/scalatest/scalatest_2.12-3.0.5.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/org/scalatest/scalatest_2.12-3.0.5.jar,,NONE)
21/08/03 13:52:10 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/org/scalatest/scalatest_2.12-3.0.5.jar,,NONE) is downloaded
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/org/scalatest/scalatest_2.12-3.0.5.jar,,NONE) as local file /local_disk0/tmp/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar in 311 milliseconds
21/08/03 13:52:11 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/org/scalatest/scalatest_2.12-3.0.5.jar,,NONE) to local file /local_disk0/tmp/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar 
21/08/03 13:52:11 INFO SparkContext: Added file /local_disk0/tmp/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar at spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar with timestamp 1627998731135
21/08/03 13:52:11 INFO Utils: Copying /local_disk0/tmp/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar
21/08/03 13:52:11 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar at spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar with timestamp 1627998731153
21/08/03 13:52:11 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/org/scalatest/scalatest_2.12-3.0.5.jar to Spark
21/08/03 13:52:11 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/org/scalatest/scalatest_2.12-3.0.5.jar
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/org/ow2/asm/asm-5.0.4.jar,,NONE))
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/org/ow2/asm/asm-5.0.4.jar,,NONE))
21/08/03 13:52:11 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/org/ow2/asm/asm-5.0.4.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/org/ow2/asm/asm-5.0.4.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/org/ow2/asm/asm-5.0.4.jar,,NONE) is downloaded
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/org/ow2/asm/asm-5.0.4.jar,,NONE) as local file /local_disk0/tmp/addedFile1052673451178424914asm_5_0_4-f9eec.jar in 32 milliseconds
21/08/03 13:52:11 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/org/ow2/asm/asm-5.0.4.jar,,NONE) to local file /local_disk0/tmp/addedFile1052673451178424914asm_5_0_4-f9eec.jar 
21/08/03 13:52:11 INFO SparkContext: Added file /local_disk0/tmp/addedFile1052673451178424914asm_5_0_4-f9eec.jar at spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar with timestamp 1627998731212
21/08/03 13:52:11 INFO Utils: Copying /local_disk0/tmp/addedFile1052673451178424914asm_5_0_4-f9eec.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile1052673451178424914asm_5_0_4-f9eec.jar
21/08/03 13:52:11 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile1052673451178424914asm_5_0_4-f9eec.jar at spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar with timestamp 1627998731249
21/08/03 13:52:11 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/org/ow2/asm/asm-5.0.4.jar to Spark
21/08/03 13:52:11 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/org/ow2/asm/asm-5.0.4.jar
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-java8-compat_2.12-0.8.0.jar,,NONE))
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-java8-compat_2.12-0.8.0.jar,,NONE))
21/08/03 13:52:11 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-java8-compat_2.12-0.8.0.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-java8-compat_2.12-0.8.0.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-java8-compat_2.12-0.8.0.jar,,NONE) is downloaded
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-java8-compat_2.12-0.8.0.jar,,NONE) as local file /local_disk0/tmp/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar in 105 milliseconds
21/08/03 13:52:11 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-java8-compat_2.12-0.8.0.jar,,NONE) to local file /local_disk0/tmp/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar 
21/08/03 13:52:11 INFO SparkContext: Added file /local_disk0/tmp/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar at spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar with timestamp 1627998731393
21/08/03 13:52:11 INFO Utils: Copying /local_disk0/tmp/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar
21/08/03 13:52:11 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar at spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar with timestamp 1627998731406
21/08/03 13:52:11 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-java8-compat_2.12-0.8.0.jar to Spark
21/08/03 13:52:11 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/org/scala-lang/modules/scala-java8-compat_2.12-0.8.0.jar
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/org/scalactic/scalactic_2.12-3.0.5.jar,,NONE))
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/org/scalactic/scalactic_2.12-3.0.5.jar,,NONE))
21/08/03 13:52:11 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/org/scalactic/scalactic_2.12-3.0.5.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/org/scalactic/scalactic_2.12-3.0.5.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/org/scalactic/scalactic_2.12-3.0.5.jar,,NONE) is downloaded
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/org/scalactic/scalactic_2.12-3.0.5.jar,,NONE) as local file /local_disk0/tmp/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar in 71 milliseconds
21/08/03 13:52:11 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/org/scalactic/scalactic_2.12-3.0.5.jar,,NONE) to local file /local_disk0/tmp/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar 
21/08/03 13:52:11 INFO SparkContext: Added file /local_disk0/tmp/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar at spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar with timestamp 1627998731503
21/08/03 13:52:11 INFO Utils: Copying /local_disk0/tmp/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar
21/08/03 13:52:11 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar at spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar with timestamp 1627998731521
21/08/03 13:52:11 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/org/scalactic/scalactic_2.12-3.0.5.jar to Spark
21/08/03 13:52:11 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/org/scalactic/scalactic_2.12-3.0.5.jar
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/minlog-1.3.0.jar,,NONE))
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/minlog-1.3.0.jar,,NONE))
21/08/03 13:52:11 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/minlog-1.3.0.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/minlog-1.3.0.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/minlog-1.3.0.jar,,NONE) is downloaded
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/minlog-1.3.0.jar,,NONE) as local file /local_disk0/tmp/addedFile6675069858488438437minlog_1_3_0-88421.jar in 34 milliseconds
21/08/03 13:52:11 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/minlog-1.3.0.jar,,NONE) to local file /local_disk0/tmp/addedFile6675069858488438437minlog_1_3_0-88421.jar 
21/08/03 13:52:11 INFO SparkContext: Added file /local_disk0/tmp/addedFile6675069858488438437minlog_1_3_0-88421.jar at spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar with timestamp 1627998731576
21/08/03 13:52:11 INFO Utils: Copying /local_disk0/tmp/addedFile6675069858488438437minlog_1_3_0-88421.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile6675069858488438437minlog_1_3_0-88421.jar
21/08/03 13:52:11 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile6675069858488438437minlog_1_3_0-88421.jar at spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar with timestamp 1627998731597
21/08/03 13:52:11 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/com/esotericsoftware/minlog-1.3.0.jar to Spark
21/08/03 13:52:11 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/com/esotericsoftware/minlog-1.3.0.jar
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j-spark_2.12-1.4.1.jar,,NONE))
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j-spark_2.12-1.4.1.jar,,NONE))
21/08/03 13:52:11 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j-spark_2.12-1.4.1.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j-spark_2.12-1.4.1.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j-spark_2.12-1.4.1.jar,,NONE) is downloaded
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j-spark_2.12-1.4.1.jar,,NONE) as local file /local_disk0/tmp/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar in 60 milliseconds
21/08/03 13:52:11 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j-spark_2.12-1.4.1.jar,,NONE) to local file /local_disk0/tmp/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar 
21/08/03 13:52:11 INFO SparkContext: Added file /local_disk0/tmp/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar at spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar with timestamp 1627998731672
21/08/03 13:52:11 INFO Utils: Copying /local_disk0/tmp/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar
21/08/03 13:52:11 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar at spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar with timestamp 1627998731683
21/08/03 13:52:11 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j-spark_2.12-1.4.1.jar to Spark
21/08/03 13:52:11 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/ml/dmlc/xgboost4j-spark_2.12-1.4.1.jar
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/kryo-4.0.2.jar,,NONE))
21/08/03 13:52:11 INFO DriverCorral: AttachLibraries - new libraries to install(including resolved dependencies): List(JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/kryo-4.0.2.jar,,NONE))
21/08/03 13:52:11 INFO SharedDriverContext: attachLibrariesToSpark JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/kryo-4.0.2.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/kryo-4.0.2.jar,,NONE)
21/08/03 13:52:11 INFO LibraryDownloadManager: Attempt 1: wait until library JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/kryo-4.0.2.jar,,NONE) is downloaded
21/08/03 13:52:11 INFO LibraryDownloadManager: Downloaded library JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/kryo-4.0.2.jar,,NONE) as local file /local_disk0/tmp/addedFile1526939427740944743kryo_4_0_2-97606.jar in 83 milliseconds
21/08/03 13:52:11 INFO SharedDriverContext: Successfully saved library JavaJarId(dbfs:/FileStore/jars/maven/com/esotericsoftware/kryo-4.0.2.jar,,NONE) to local file /local_disk0/tmp/addedFile1526939427740944743kryo_4_0_2-97606.jar 
21/08/03 13:52:11 INFO SparkContext: Added file /local_disk0/tmp/addedFile1526939427740944743kryo_4_0_2-97606.jar at spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar with timestamp 1627998731785
21/08/03 13:52:11 INFO Utils: Copying /local_disk0/tmp/addedFile1526939427740944743kryo_4_0_2-97606.jar to /local_disk0/spark-070bf151-f802-4dcc-a63e-a822a67456a3/userFiles-4c32caa4-642d-4bb8-8f70-e643c49179e3/addedFile1526939427740944743kryo_4_0_2-97606.jar
21/08/03 13:52:11 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile1526939427740944743kryo_4_0_2-97606.jar at spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar with timestamp 1627998731896
21/08/03 13:52:11 INFO SharedDriverContext: Successfully attached library dbfs:/FileStore/jars/maven/com/esotericsoftware/kryo-4.0.2.jar to Spark
21/08/03 13:52:11 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/maven/com/esotericsoftware/kryo-4.0.2.jar
21/08/03 13:52:16 INFO Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)
21/08/03 13:52:22 INFO RDriverLocal$: SparkR installation completed.
21/08/03 13:52:22 INFO RDriverLocal: 5. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: launching R process ...
21/08/03 13:52:22 INFO RDriverLocal: 6. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: cgroup isolation disabled, not placing R process in REPL cgroup.
21/08/03 13:52:22 INFO RDriverLocal: 7. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: starting R process on port 1100 (attempt 1) ...
21/08/03 13:52:22 INFO RDriverLocal: 8. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: setting up BufferedStreamThread with bufferSize: 1000.
21/08/03 13:52:24 INFO RDriverLocal: 9. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: R process started with RServe listening on port 1100.
21/08/03 13:52:24 INFO RDriverLocal: 10. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: starting interpreter to talk to R process ...
21/08/03 13:52:24 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
21/08/03 13:52:24 INFO ROutputStreamHandler: Successfully connected to stdout in the RShell.
21/08/03 13:52:24 INFO ROutputStreamHandler: Successfully connected to stderr in the RShell.
21/08/03 13:52:24 INFO RDriverLocal: 11. RDriverLocal.222c979d-0f4f-4429-99a6-7b502296f8ed: R interpretter is connected.
21/08/03 13:52:25 INFO RDriverWrapper: setupRepl:ReplId-506c9-2260b-72db4-3: finished to load
21/08/03 13:56:56 INFO HikariDataSource: metastore-monitor - Starting...
21/08/03 13:56:56 INFO HikariDataSource: metastore-monitor - Start completed.
21/08/03 13:56:56 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
21/08/03 13:56:56 INFO HikariDataSource: metastore-monitor - Shutdown completed.
21/08/03 13:56:56 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 110 milliseconds)
21/08/03 13:57:06 INFO DriverCorral: DBFS health check ok
21/08/03 13:57:06 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:57:06 INFO HiveUtils: Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/hive/third_party--gcs-private--proto-google-iam-v1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.655.jar:file:/databricks/hive/dbfs--exceptions--exceptions-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl__0.22.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/extern--acl--auth--auth-spark_3.1_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/third_party--gcs-private--google-oauth-client-java6_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_liball_deps_2.12_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.30.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.4.36.v20210114.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-library_2.12--org.scala-lang__scala-library__2.12.10.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.10.0.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.4.36.v20210114.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http_io.netty__netty-codec-http__4.1.63.Final_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-transport-native-unix-common-linux-x86_64_io.netty__netty-transport-native-unix-common-linux-x86_64__4.1.63.Final_shaded.jar:file:/databricks/hive/third_party--gcs-private--util_shaded.jar:file:/databricks/hive/common--jetty--client--client-spark_3.1_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-impl-core__0.22.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--joda-time--joda-time--joda-time__joda-time__2.10.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-client__0.33.1_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okhttp3__okhttp__3.9.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.json--json--org.json__json__20090211.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.8.2.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_javax.annotation_javax.annotation-api_javax.annotation__javax.annotation-api__1.3.2_shaded.jar:file:/databricks/hive/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-handler-proxy_io.netty__netty-handler-proxy__4.1.63.Final_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-common_io.netty__netty-common__4.1.63.Final_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__4.1.1.jar:file:/databricks/hive/third_party--hadoop--hadoop-tools--hadoop-aws--lib-spark_3.1_2.12_deploy_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient--io.prometheus__simpleclient__0.7.0.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:file:/databricks/hive/third_party--datalake--datalake-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-context_io.grpc__grpc-context__1.36.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar:file:/databricks/hive/extern--libaws-regions.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-handler_io.netty__netty-handler__4.1.63.Final_shaded.jar:file:/databricks/hive/third_party--gcs-private--google-http-client-jackson2_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.code.findbugs_jsr305_com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.hdrhistogram_HdrHistogram_org.hdrhistogram__HdrHistogram__2.1.12_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.1.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/common--tracing--tracing-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.30.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec_io.netty__netty-codec__4.1.63.Final_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--compilerplugin_2.12--com.databricks.scalapb__compilerplugin_2.12__0.4.15-10.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--junit--junit--junit__junit__3.8.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-client-runtime__1.7.12_container_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-netty-shaded_shaded.jar:file:/databricks/hive/third_party--gcs-private--auto-value-annotations_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.checkerframework_checker-compat-qual_org.checkerframework__checker-compat-qual__2.5.5_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.4.36.v20210114.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-dns_io.netty__netty-codec-dns__4.1.63.Final_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/hive/logging--log4j-mod--log4j-mod-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--gcs-private--grpc-protobuf_shaded.jar:file:/databricks/hive/common--lazy--lazy-spark_3.1_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--oro--oro--oro__oro__2.0.8.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.655.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/third_party--jackson--jackson-module-scala-shaded_2.12_deploy.jar:file:/databricks/hive/third_party--jetty-client--jetty-util_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:file:/databricks/hive/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-http2_io.netty__netty-codec-http2__4.1.63.Final_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jakarta.validation--jakarta.validation-api--jakarta.validation__jakarta.validation-api__2.0.2.jar:file:/databricks/hive/third_party--gcs-private--google-auth-library-credentials_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__4.1.1.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__fastparse_2.12__2.1.3_shaded.jar:file:/databricks/hive/daemon--data--client--conf--conf-spark_3.1_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/jsonutil--jsonutil-spark_3.1_2.12_deploy.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:file:/databricks/hive/daemon--data--data-common--data-common-spark_3.1_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.655.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__4.1.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/hive/third_party--gcs-private--grpc-auth_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.10.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-haproxy_io.netty__netty-codec-haproxy__4.1.63.Final_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar:file:/databricks/hive/third_party--gcs-private--httpclient_shaded.jar:file:/databricks/hive/third_party--gcs-private--google-api-client-jackson2_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.4.36.v20210114.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-api__0.31.0_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-api_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.4.36.v20210114.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-core__0.33.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang--scala-reflect_2.12--org.scala-lang__scala-reflect__2.12.10.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/hive/third_party--gcs-private--util-hadoop_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf-lite_io.grpc__grpc-protobuf-lite__1.36.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--jline--jline--jline__jline__0.9.94.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver_io.netty__netty-resolver__4.1.63.Final_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalatest--scalatest_2.12--org.scalatest__scalatest_2.12__3.0.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:file:/databricks/hive/common--util--locks-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--gcs-private--google-extensions_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/hive/common--credentials--credentials-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--jackson--guava_only_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-services_io.grpc__grpc-services__1.36.1_shaded.jar:file:/databricks/hive/third_party--gcs-private--google-http-client_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/hive/extern--extern-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc_com.linecorp.armeria__armeria-grpc__1.6.0_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria_com.linecorp.armeria__armeria__1.6.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--io.netty--netty--io.netty__netty__3.8.0.Final.jar:file:/databricks/hive/third_party--opencensus-shaded--com.squareup.okio__okio__1.13.0_shaded.jar:file:/databricks/hive/third_party--gcs-private--protobuf-java-util_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--opencensus-shaded--com.lmax__disruptor__3.4.2_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe.scala-logging--scala-logging_2.12--com.typesafe.scala-logging__scala-logging_2.12__3.7.2.jar:file:/databricks/hive/third_party--gcs-private--commons-logging_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-noop__0.31.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.20.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.4.36.v20210114.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/third_party--gcs-private--google-auth-library-oauth2-http_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.linecorp.armeria_armeria-grpc-protocol_com.linecorp.armeria__armeria-grpc-protocol__1.6.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.655.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.errorprone_error_prone_annotations_com.google.errorprone__error_prone_annotations__2.4.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.trueaccord.lenses--lenses_2.12--com.trueaccord.lenses__lenses_2.12__0.4.12.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-codec-socks_io.netty__netty-codec-socks__4.1.63.Final_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.api.grpc_proto-google-common-protos_com.google.api.grpc__proto-google-common-protos__2.0.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar:file:/databricks/hive/third_party--opencensus-shaded--org.checkerframework__checker-compat-qual__2.5.2_shaded.jar:file:/databricks/hive/third_party--gcs-private--opencensus-contrib-http-util_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.guava_listenablefuture_com.google.guava__listenablefuture__9999.0-empty-to-avoid-conflict-with-guava_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-thrift__0.33.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-annotations_com.fasterxml.jackson.core__jackson-annotations__2.12.2_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-resolver-dns_io.netty__netty-resolver-dns__4.1.63.Final_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.grpc__grpc-context__1.19.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.4.36.v20210114.jar:file:/databricks/hive/third_party--gcs-private--grpc-stub_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-xml_2.12--org.scala-lang.modules__scala-xml_2.12__1.2.0.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.gson__gson__2.8.2_shaded.jar:file:/databricks/hive/third_party--gcs-private--commons-lang3_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.databricks.scalapb--scalapb-runtime_2.12--com.databricks.scalapb__scalapb-runtime_2.12__0.4.15-10.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--jackson--jsr305_only_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:file:/databricks/hive/third_party--gcs-private--google-api-services-storage_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.guava_failureaccess_com.google.guava__failureaccess__1.0.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.4.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-jaeger__0.22.1_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/dbfs--utils--dbfs-utils-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java-util_com.google.protobuf__protobuf-java-util__3.12.0_shaded.jar:file:/databricks/hive/third_party--gcs-private--gcs-shaded-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing__opentracing-util__0.31.0_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-api__0.22.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/hive/----jackson_annotations_shaded--libjackson-annotations.jar:file:/databricks/hive/third_party--gcs-private--flogger_shaded.jar:file:/databricks/hive/s3commit--client--client-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--org.codehaus.mojo__animal-sniffer-annotations__1.14_shaded.jar:file:/databricks/hive/api-base--api-base-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.codehaus.mojo_animal-sniffer-annotations_org.codehaus.mojo__animal-sniffer-annotations__1.19_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml--classmate--com.fasterxml__classmate__1.3.4.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar:file:/databricks/hive/daemon--data--client--client-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpclient__4.4.1_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.micrometer_micrometer-core_io.micrometer__micrometer-core__1.6.5_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-io--commons-io--commons-io__commons-io__2.4.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:file:/databricks/hive/third_party--dropwizard-metrics-log4j-v3.2.6--metrics-log4j-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-stub_io.grpc__grpc-stub__1.36.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:file:/databricks/hive/third_party--gcs-private--perfmark-api_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-context_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.4.36.v20210114.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-jvm_2.12--com.twitter__util-jvm_2.12__7.1.0.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.j2objc__j2objc-annotations__1.1_shaded.jar:file:/databricks/hive/common--reflection--reflection-spark_3.1_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-jackson--org.apache.parquet__parquet-jackson__1.10.1-databricks9.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.10.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.latencyutils_LatencyUtils_org.latencyutils__LatencyUtils__2.0.3_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.12_shaded_20180920_b33d810_spark_3.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-core_io.grpc__grpc-core__1.36.1_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.code.findbugs__jsr305__3.0.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/----scalapb_090--runtime-unshaded-jetty9-hadoop1_2.12_deploy_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-protobuf-lite_shaded.jar:file:/databricks/hive/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:file:/databricks/hive/common--path--path-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.errorprone__error_prone_annotations__2.1.3_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.reactivestreams_reactive-streams_org.reactivestreams__reactive-streams__1.0.3_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.joda_joda-convert_org.joda__joda-convert__2.2.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-fileupload--commons-fileupload--commons-fileupload__commons-fileupload__1.3.3.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:file:/databricks/hive/third_party--gcs-private--gcs-connector_shaded.jar:file:/databricks/hive/third_party--gcs-private--google-api-services-iamcredentials_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/third_party--jackson--paranamer_only_shaded.jar:file:/databricks/hive/----jackson_databind_shaded--libjackson-databind.jar:file:/databricks/hive/s3commit--common--common-spark_3.1_2.12_deploy.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-common--org.apache.parquet__parquet-common__1.10.1-databricks9.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.12_shaded_20180920_b33d810_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-net--commons-net--commons-net__commons-net__3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.protobuf_protobuf-java_com.google.protobuf__protobuf-java__3.14.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/third_party--gcs-private--grpc-core_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.3.2.Final.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/third_party--gcs-private--gcsio_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scala-lang.modules--scala-parser-combinators_2.12--org.scala-lang.modules__scala-parser-combinators_2.12__1.1.2.jar:file:/databricks/hive/third_party--gcs-private--checker-compat-qual_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.prometheus--simpleclient_dropwizard--io.prometheus__simpleclient_dropwizard__0.7.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-app_2.12--com.twitter__util-app_2.12__7.1.0.jar:file:/databricks/hive/common--client--client-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--gcs-private--opencensus-api_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.10.0.jar:file:/databricks/hive/third_party--gcs-private--proto-google-common-protos_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--gcs-private--javax.annotation-api_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.typesafe--config--com.typesafe__config__1.2.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-databind_com.fasterxml.jackson.core__jackson-databind__2.12.2_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-column--org.apache.parquet__parquet-column__1.10.1-databricks9.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.httpcomponents__httpcore__4.4.1_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--io.jaegertracing__jaeger-tracerresolver__0.33.1_shaded.jar:file:/databricks/hive/third_party--gcs-private--animal-sniffer-annotations_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-jmx--io.dropwizard.metrics__metrics-jmx__4.1.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.4.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-lint_2.12--com.twitter__util-lint_2.12__7.1.0.jar:file:/databricks/hive/third_party--gcs-private--flogger-slf4j-backend_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.hibernate.validator--hibernate-validator--org.hibernate.validator__hibernate-validator__6.1.0.Final.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180920_b33d810_spark_3.1.jar:file:/databricks/hive/third_party--gcs-private--j2objc-annotations_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/common--hadoop--hadoop-spark_3.1_2.12_deploy.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-encoding--org.apache.parquet__parquet-encoding__1.10.1-databricks9.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_3.1_2.12_deploy_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.4.36.v20210114.jar:file:/databricks/hive/third_party--gcs-private--flogger-system-backend_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--jets3t-0.7--liball_deps_2.12.jar:file:/databricks/hive/third_party--gcs-private--guava_shaded.jar:file:/databricks/hive/third_party--gcs-private--httpcore_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-core_2.12--com.twitter__util-core_2.12__7.1.0.jar:file:/databricks/hive/third_party--gcs-private--gson_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/hive/s3--s3-spark_3.1_2.12_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/hive/third_party--gcs-private--protobuf-java_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:file:/databricks/hive/third_party--gcs-private--google-api-client_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-logging__commons-logging__1.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-pool--commons-pool--commons-pool__commons-pool__1.5.4.jar:file:/databricks/hive/macros--ratelimitedlogger--ratelimitedlogger-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-protobuf_io.grpc__grpc-protobuf__1.36.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.github.ben-manes.caffeine--caffeine--com.github.ben-manes.caffeine__caffeine__2.3.4.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.10.0.jar:file:/databricks/hive/third_party--gcs-private--failureaccess_shaded.jar:file:/databricks/hive/third_party--gcs-private--google-oauth-client_shaded.jar:file:/databricks/hive/third_party--opencensus-shaded--commons-codec__commons-codec__1.9_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.10.0.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-function_2.12--com.twitter__util-function_2.12__7.1.0.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--javolution--javolution--javolution__javolution__5.5.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--gcs-private--checker-qual_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_net.bytebuddy_byte-buddy_net.bytebuddy__byte-buddy__1.10.19_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__4.1.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.helger--profiler--com.helger__profiler__1.1.1.jar:file:/databricks/hive/third_party--jetty-client--jetty-client_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.guava--guava--com.google.guava__guava__15.0.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-storage__8.6.4_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.grpc_grpc-api_io.grpc__grpc-api__1.36.1_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.guava_guava_com.google.guava__guava__30.0-android_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-buffer_io.netty__netty-buffer__4.1.63.Final_shaded.jar:file:/databricks/hive/third_party--azure--com.microsoft.rest__client-runtime__1.7.12_container_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opencensus__opencensus-exporter-trace-util__0.22.1_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__3.0.0.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.6.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.android_annotations_com.google.android__annotations__4.1.1.4_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.netty--netty-all--io.netty__netty-all__4.1.51.Final.jar:file:/databricks/hive/common--storage-driver-utils--storage_driver_utils-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--gcs-private--jackson-core_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-hadoop--org.apache.parquet__parquet-hadoop__1.10.1-databricks9.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.12_shaded_20180920_b33d810_spark_3.1.jar:file:/databricks/hive/macros--sourcecode--sourcecode-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.j2objc_j2objc-annotations_com.google.j2objc__j2objc-annotations__1.3_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.google.code.gson_gson_com.google.code.gson__gson__2.8.6_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.12.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--gcs-private--google-api-client-java6_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar:file:/databricks/hive/----scalapb_090--com.lihaoyi__sourcecode_2.12__0.1.7_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.perfmark_perfmark-api_io.perfmark__perfmark-api__0.23.0_shaded.jar:file:/databricks/hive/third_party--jetty-client--jetty-http_shaded.jar:file:/databricks/hive/third_party--gcs-private--jsr305_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_3.1_2.12_deploy_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-alts_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.1.1.jar:file:/databricks/hive/third_party--hadoop_azure_abfs--hadoop-tools--hadoop-azure--lib-spark_3.1_2.12_deploy.jar_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--gcs-private--error_prone_annotations_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.4.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.scalactic--scalactic_2.12--org.scalactic__scalactic_2.12__3.0.8.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:file:/databricks/hive/third_party--opencensus-shaded--com.google.guava__guava__26.0-android_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:file:/databricks/hive/----workspace_spark_3_1--vendor--spark-ganglia-lgpl--libmetrics-ganglia.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:file:/databricks/hive/third_party--gcs-private--api-common_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.twitter--util-stats_2.12--com.twitter__util-stats_2.12__7.1.0.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.4.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_com.fasterxml.jackson.core_jackson-core_com.fasterxml.jackson.core__jackson-core__2.12.2_shaded.jar:file:/databricks/hive/api-base--api-base_java-spark_3.1_2.12_deploy.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_org.curioswitch.curiostack_protobuf-jackson_org.curioswitch.curiostack__protobuf-jackson__1.2.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.4.36.v20210114.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.parquet--parquet-format--org.apache.parquet__parquet-format__2.4.0.jar:file:/databricks/hive/----jackson_core_shaded--libjackson-core.jar:file:/databricks/hive/third_party--opencensus-shaded--org.apache.thrift__libthrift__0.11.0_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.eclipse.jetty--jetty-util-ajax--org.eclipse.jetty__jetty-util-ajax__9.4.36.v20210114.jar:file:/databricks/hive/third_party--gcs-private--conscrypt-openjdk-uber_shaded.jar:file:/databricks/hive/third_party--armeria--maven-trees_armeria_io.netty_netty-transport_io.netty__netty-transport__4.1.63.Final_shaded.jar:file:/databricks/hive/third_party--jetty-client--jetty-io_shaded.jar:file:/databricks/hive/third_party--gcs-private--listenablefuture_shaded.jar:file:/databricks/hive/third_party--gcs-private--commons-codec_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/databricks/hive/spark--maven-trees--spark_1.4_hive_0.13--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.wildfly.openssl--wildfly-openssl--org.wildfly.openssl__wildfly-openssl__1.0.7.Final.jar:file:/databricks/hive/third_party--gcs-private--annotations_shaded.jar:file:/databricks/hive/third_party--gcs-private--grpc-grpclb_shaded.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.655.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__8.6.4_2.12_shaded_20180625_3682417_spark_3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/----workspace_spark_3_1--maven-trees--hive-2.3__hadoop-2.7--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:file:/databricks/hive/third_party--opencensus-shaded--io.opentracing.contrib__opentracing-tracerresolver__0.1.5_shaded.jar:file:/databricks/hive/bonecp-configs.jar
21/08/03 13:57:07 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21/08/03 13:57:07 INFO ObjectStore: ObjectStore, initialize called
21/08/03 13:57:07 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/08/03 13:57:07 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
21/08/03 13:57:07 INFO Persistence: Property datanucleus.schema.autoCreateAll unknown - will be ignored
21/08/03 13:57:08 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/08/03 13:57:11 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
21/08/03 13:57:11 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
21/08/03 13:57:11 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
21/08/03 13:57:11 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
21/08/03 13:57:12 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21/08/03 13:57:12 INFO ObjectStore: Initialized ObjectStore
21/08/03 13:57:12 INFO HiveMetaStore: Added admin role in metastore
21/08/03 13:57:12 INFO HiveMetaStore: Added public role in metastore
21/08/03 13:57:12 INFO HiveMetaStore: No user is added in admin role, since config is empty
21/08/03 13:57:12 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
21/08/03 13:57:12 INFO HiveClientImpl: Warehouse location for Hive client (version 0.13.1) is /user/hive/warehouse
21/08/03 13:57:12 INFO HiveMetaStore: 0: get_database: default
21/08/03 13:57:12 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21/08/03 13:57:12 INFO HiveMetaStore: 0: get_database: default
21/08/03 13:57:12 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21/08/03 13:57:12 INFO DriverCorral: Metastore health check ok
21/08/03 13:57:29 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:57:29 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:57:29 INFO DriverCorral: Starting scala repl ReplId-4997b-51fc6-fbbdc-d
21/08/03 13:57:29 WARN ScalaDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/08/03 13:57:29 INFO ClusterLoadMonitor: Added query with execution ID:0. Current active queries:1
21/08/03 13:57:29 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
21/08/03 13:57:29 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:57:29 WARN SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead.
21/08/03 13:57:29 INFO DriverCorral: Starting sql repl ReplId-22ada-20429-9c274-3
21/08/03 13:57:29 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:1. Current active queries:2
21/08/03 13:57:30 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:0. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:1. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:2. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:3. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:3. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:2. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:4. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:5. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:4. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:5. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:6. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:7. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile6675069858488438437minlog_1_3_0-88421.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:7. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile6675069858488438437minlog_1_3_0-88421.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:6. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:8. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:9. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile1052673451178424914asm_5_0_4-f9eec.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:9. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile1052673451178424914asm_5_0_4-f9eec.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:8. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:11. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:10. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:10. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:11. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:12. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:13. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:12. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:13. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:14. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile1526939427740944743kryo_4_0_2-97606.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:14. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:15. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:16. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile1526939427740944743kryo_4_0_2-97606.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:15. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:16. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:17. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:18. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:17. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:18. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:19. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:20. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile8241248691510676812commons_logging_1_2-0b642.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:20. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:19. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:21. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile1775982106135659717config_1_3_3-49c52.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:21. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:22. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:23. Current active queries:2
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile8241248691510676812commons_logging_1_2-0b642.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:23. Current active queries:1
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:24. Current active queries:2
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:22. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:24. Current active queries:0
21/08/03 13:57:30 INFO ScalaDriverWrapper: setupRepl:ReplId-4997b-51fc6-fbbdc-d: finished to load
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:25. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile1775982106135659717config_1_3_3-49c52.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:25. Current active queries:0
21/08/03 13:57:30 INFO ClusterLoadMonitor: Added query with execution ID:26. Current active queries:1
21/08/03 13:57:30 WARN SparkContext: The jar /local_disk0/tmp/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:30 INFO ClusterLoadMonitor: Removed query with execution ID:26. Current active queries:0
21/08/03 13:57:31 INFO ClusterLoadMonitor: Added query with execution ID:27. Current active queries:1
21/08/03 13:57:31 WARN SparkContext: The jar /local_disk0/tmp/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar has been added already. Overwriting of added jars is not supported in the current version.
21/08/03 13:57:31 INFO ClusterLoadMonitor: Removed query with execution ID:27. Current active queries:0
21/08/03 13:57:31 INFO SQLDriverWrapper: setupRepl:ReplId-22ada-20429-9c274-3: finished to load
21/08/03 13:57:31 INFO ProgressReporter$: Added result fetcher for 5302906234339048909_8057104574846571832_fb011d46071d49fb85b417f33f4e599b
21/08/03 13:57:31 INFO ProgressReporter$: Added result fetcher for 2498831507022161731_4684770857255736199_e7a71f5e-17b1-4477-9180-6c6940a399e5
21/08/03 13:57:31 INFO SignalUtils: Registering signal handler for INT
21/08/03 13:57:31 INFO AsyncEventQueue: Process of event SparkListenerSQLUsageLogging(1,1627999050608,CallSite(sql at DriverLocal.scala:211,org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
com.databricks.backend.daemon.driver.DriverLocal.$anonfun$new$3(DriverLocal.scala:211)
com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1240)
com.databricks.backend.daemon.driver.DriverLocal.$anonfun$new$2(DriverLocal.scala:211)
scala.collection.Iterator.foreach(Iterator.scala:941)
scala.collection.Iterator.foreach$(Iterator.scala:941)
scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
scala.collection.IterableLike.foreach(IterableLike.scala:74)
scala.collection.IterableLike.foreach$(IterableLike.scala:73)
scala.collection.AbstractIterable.foreach(Iterable.scala:56)
com.databricks.backend.daemon.driver.DriverLocal.<init>(DriverLocal.scala:195)
com.databricks.backend.daemon.driver.SQLDriverLocal.<init>(SQLDriverLocal.scala:24)
com.databricks.backend.daemon.driver.SQLDriverWrapper.instantiateDriver(DriverWrapper.scala:859)
com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:331)
com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:220)
java.lang.Thread.run(Thread.java:748)),org.apache.spark.sql.internal.SQLConf@47297eff,Some(CommandContext(Map(),Map())),== Parsed Logical Plan ==
AddJarCommand /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar

== Analyzed Logical Plan ==
result: int
AddJarCommand /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar

== Optimized Logical Plan ==
AddJarCommand /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar

== Physical Plan ==
Execute AddJarCommand
   +- AddJarCommand /local_disk0/tmp/addedFile8305477370858712955objenesis_2_5_1-7628f.jar
,None,None,None,None) by listener SQLAppStatusListener took 1.06126s.
21/08/03 13:57:31 INFO ClusterLoadMonitor: Added query with execution ID:28. Current active queries:1
21/08/03 13:57:31 INFO HiveMetaStore: 1: get_databases: *
21/08/03 13:57:31 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: *	
21/08/03 13:57:31 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
21/08/03 13:57:31 INFO ObjectStore: ObjectStore, initialize called
21/08/03 13:57:32 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
21/08/03 13:57:32 INFO ObjectStore: Initialized ObjectStore
21/08/03 13:57:32 INFO DriverILoop: Set class prefix to: $line26b7a5959a12454694f9bfa15f0ab9a7
21/08/03 13:57:32 INFO DriverILoop: set ContextClassLoader
21/08/03 13:57:32 INFO DriverILoop: initialized intp
21/08/03 13:57:32 INFO CodeGenerator: Code generated in 375.6803 ms
21/08/03 13:57:32 INFO ClusterLoadMonitor: Removed query with execution ID:28. Current active queries:0
21/08/03 13:57:32 INFO ClusterLoadMonitor: Added query with execution ID:29. Current active queries:1
21/08/03 13:57:32 INFO ClusterLoadMonitor: Removed query with execution ID:29. Current active queries:0
21/08/03 13:57:32 INFO ClusterLoadMonitor: Added query with execution ID:30. Current active queries:1
21/08/03 13:57:32 INFO CodeGenerator: Code generated in 32.6704 ms
21/08/03 13:57:32 INFO ClusterLoadMonitor: Removed query with execution ID:30. Current active queries:0
21/08/03 13:57:33 INFO CodeGenerator: Code generated in 38.4644 ms
21/08/03 13:57:33 INFO ProgressReporter$: Removed result fetcher for 2498831507022161731_4684770857255736199_e7a71f5e-17b1-4477-9180-6c6940a399e5
21/08/03 13:57:33 INFO ProgressReporter$: Added result fetcher for 2498831507022161731_5658308091455428989_eccde8bc-b720-4e12-a734-925d9ccf5786
21/08/03 13:57:34 WARN SimpleFunctionRegistry: The function getargument replaced a previously registered function.
21/08/03 13:57:34 INFO ClusterLoadMonitor: Added query with execution ID:31. Current active queries:1
21/08/03 13:57:34 INFO HiveMetaStore: 1: get_database: global_temp
21/08/03 13:57:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/08/03 13:57:34 ERROR RetryingHMSHandler: NoSuchObjectException(message:There is no database named global_temp)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMDatabase(ObjectStore.java:487)
	at org.apache.hadoop.hive.metastore.ObjectStore.getDatabase(ObjectStore.java:498)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy46.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_database(HiveMetaStore.java:796)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy47.get_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabase(HiveMetaStoreClient.java:949)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy48.getDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getDatabase(Hive.java:1165)
	at org.apache.hadoop.hive.ql.metadata.Hive.databaseExists(Hive.java:1154)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$databaseExists$1(HiveClientImpl.scala:441)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:348)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$retryLocked$1(HiveClientImpl.scala:251)
	at org.apache.spark.sql.hive.client.HiveClientImpl.synchronizeOnObject(HiveClientImpl.scala:287)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:243)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:330)
	at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:441)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$databaseExists$1(PoolingHiveClient.scala:274)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$databaseExists$1$adapted(PoolingHiveClient.scala:273)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.withHiveClient(PoolingHiveClient.scala:112)
	at org.apache.spark.sql.hive.client.PoolingHiveClient.databaseExists(PoolingHiveClient.scala:273)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:292)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$2(HiveExternalCatalog.scala:151)
	at org.apache.spark.sql.hive.HiveExternalCatalog.maybeSynchronized(HiveExternalCatalog.scala:112)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$1(HiveExternalCatalog.scala:150)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:377)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:363)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:149)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:292)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.databaseExists(ExternalCatalogWithListener.scala:77)
	at org.apache.spark.sql.internal.SharedState.$anonfun$globalTempViewManager$1(SharedState.scala:199)
	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:199)
	at org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:196)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:66)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:124)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:124)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.doListTables(SessionCatalog.scala:1114)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1064)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1051)
	at org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$46(tables.scala:914)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:914)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:233)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3802)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3800)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:233)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:687)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.$anonfun$executeSql$1(SQLDriverLocal.scala:91)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.executeSql(SQLDriverLocal.scala:37)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.repl(SQLDriverLocal.scala:144)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$13(DriverLocal.scala:544)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:53)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:53)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:521)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:689)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:681)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:522)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:634)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:427)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:370)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:221)
	at java.lang.Thread.run(Thread.java:748)

21/08/03 13:57:34 INFO HiveMetaStore: 1: get_database: default
21/08/03 13:57:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21/08/03 13:57:34 INFO HiveMetaStore: 1: get_database: default
21/08/03 13:57:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
21/08/03 13:57:34 INFO HiveMetaStore: 1: get_tables: db=default pat=*
21/08/03 13:57:34 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/08/03 13:57:34 INFO ClusterLoadMonitor: Removed query with execution ID:31. Current active queries:0
21/08/03 13:57:34 INFO ClusterLoadMonitor: Added query with execution ID:32. Current active queries:1
21/08/03 13:57:34 INFO ClusterLoadMonitor: Removed query with execution ID:32. Current active queries:0
21/08/03 13:57:34 INFO ClusterLoadMonitor: Added query with execution ID:33. Current active queries:1
21/08/03 13:57:34 INFO CodeGenerator: Code generated in 9.8758 ms
21/08/03 13:57:34 INFO ClusterLoadMonitor: Removed query with execution ID:33. Current active queries:0
21/08/03 13:57:34 INFO CodeGenerator: Code generated in 14.5387 ms
21/08/03 13:57:34 INFO ProgressReporter$: Removed result fetcher for 2498831507022161731_5658308091455428989_eccde8bc-b720-4e12-a734-925d9ccf5786
21/08/03 13:57:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:36 INFO SparkContext: Created broadcast 0 from textFile at ReadWrite.scala:587
21/08/03 13:57:37 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:37 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:37 INFO DAGScheduler: Got job 0 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:37 INFO DAGScheduler: Final stage: ResultStage 0 (first at ReadWrite.scala:587)
21/08/03 13:57:37 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:37 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:37 INFO DAGScheduler: Submitting ResultStage 0 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/metadata MapPartitionsRDD[1] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:37 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:37 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:37 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/metadata MapPartitionsRDD[1] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
21/08/03 13:57:37 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 5302906234339048909, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 5302906234339048909. Created 5302906234339048909 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
21/08/03 13:57:37 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool 5302906234339048909
21/08/03 13:57:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.48.11.72, executor 1, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 21.8 KiB, free 3.3 GiB)
21/08/03 13:57:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 3.3 GiB)
21/08/03 13:57:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.48.11.69:43057 (size: 5.8 KiB, free: 3.3 GiB)
21/08/03 13:57:37 INFO SparkContext: Created broadcast 1 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.48.11.72:45133 (size: 5.8 KiB, free: 4.6 GiB)
21/08/03 13:57:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.48.11.72:45133 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3220 ms on 10.48.11.72 (executor 1) (1/1)
21/08/03 13:57:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:40 INFO DAGScheduler: ResultStage 0 (first at ReadWrite.scala:587) finished in 3.306 s
21/08/03 13:57:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
21/08/03 13:57:40 INFO DAGScheduler: Job 0 finished: first at ReadWrite.scala:587, took 3.603260 s
21/08/03 13:57:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:40 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:40 INFO SparkContext: Created broadcast 2 from textFile at ReadWrite.scala:587
21/08/03 13:57:40 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:40 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:40 INFO DAGScheduler: Got job 1 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:40 INFO DAGScheduler: Final stage: ResultStage 1 (first at ReadWrite.scala:587)
21/08/03 13:57:40 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:40 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:40 INFO DAGScheduler: Submitting ResultStage 1 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/evaluator/metadata MapPartitionsRDD[3] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:40 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:40 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:40 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/evaluator/metadata MapPartitionsRDD[3] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
21/08/03 13:57:40 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool 5302906234339048909
21/08/03 13:57:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.48.11.74, executor 5, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.8 KiB, free 3.3 GiB)
21/08/03 13:57:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 3.3 GiB)
21/08/03 13:57:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.48.11.69:43057 (size: 5.8 KiB, free: 3.3 GiB)
21/08/03 13:57:40 INFO SparkContext: Created broadcast 3 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.48.11.74:35137 (size: 5.8 KiB, free: 4.6 GiB)
21/08/03 13:57:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.48.11.74:35137 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2998 ms on 10.48.11.74 (executor 5) (1/1)
21/08/03 13:57:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:43 INFO DAGScheduler: ResultStage 1 (first at ReadWrite.scala:587) finished in 3.001 s
21/08/03 13:57:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/08/03 13:57:43 INFO DAGScheduler: Job 1 finished: first at ReadWrite.scala:587, took 3.005605 s
21/08/03 13:57:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:43 INFO SparkContext: Created broadcast 4 from textFile at ReadWrite.scala:587
21/08/03 13:57:44 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:44 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:44 INFO DAGScheduler: Got job 2 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:44 INFO DAGScheduler: Final stage: ResultStage 2 (first at ReadWrite.scala:587)
21/08/03 13:57:44 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:44 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:44 INFO DAGScheduler: Submitting ResultStage 2 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/evaluator/metadata MapPartitionsRDD[5] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:44 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:44 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:44 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/evaluator/metadata MapPartitionsRDD[5] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
21/08/03 13:57:44 WARN FairSchedulableBuilder: A job was submitted with scheduler pool 5302906234339048909, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 5302906234339048909. Created 5302906234339048909 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)
21/08/03 13:57:44 INFO FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool 5302906234339048909
21/08/03 13:57:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.48.11.71, executor 3, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 21.8 KiB, free 3.3 GiB)
21/08/03 13:57:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 3.3 GiB)
21/08/03 13:57:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.48.11.69:43057 (size: 5.8 KiB, free: 3.3 GiB)
21/08/03 13:57:44 INFO SparkContext: Created broadcast 5 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.48.11.71:46151 (size: 5.8 KiB, free: 4.6 GiB)
21/08/03 13:57:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.48.11.71:46151 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2952 ms on 10.48.11.71 (executor 3) (1/1)
21/08/03 13:57:46 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:46 INFO DAGScheduler: ResultStage 2 (first at ReadWrite.scala:587) finished in 2.957 s
21/08/03 13:57:46 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
21/08/03 13:57:46 INFO DAGScheduler: Job 2 finished: first at ReadWrite.scala:587, took 2.961844 s
21/08/03 13:57:47 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:47 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:47 INFO SparkContext: Created broadcast 6 from textFile at ReadWrite.scala:587
21/08/03 13:57:47 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:47 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:47 INFO DAGScheduler: Got job 3 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:47 INFO DAGScheduler: Final stage: ResultStage 3 (first at ReadWrite.scala:587)
21/08/03 13:57:47 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:47 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:47 INFO DAGScheduler: Submitting ResultStage 3 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/metadata MapPartitionsRDD[7] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:47 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:47 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:47 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/metadata MapPartitionsRDD[7] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:47 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
21/08/03 13:57:47 INFO FairSchedulableBuilder: Added task set TaskSet_3.0 tasks to pool 5302906234339048909
21/08/03 13:57:47 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.48.11.71, executor 3, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:47 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.8 KiB, free 3.3 GiB)
21/08/03 13:57:47 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 3.3 GiB)
21/08/03 13:57:47 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.48.11.69:43057 (size: 5.8 KiB, free: 3.3 GiB)
21/08/03 13:57:47 INFO SparkContext: Created broadcast 7 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:47 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.48.11.71:46151 (size: 5.8 KiB, free: 4.6 GiB)
21/08/03 13:57:47 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.48.11.71:46151 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:47 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 112 ms on 10.48.11.71 (executor 3) (1/1)
21/08/03 13:57:47 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:47 INFO DAGScheduler: ResultStage 3 (first at ReadWrite.scala:587) finished in 0.115 s
21/08/03 13:57:47 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/08/03 13:57:47 INFO DAGScheduler: Job 3 finished: first at ReadWrite.scala:587, took 0.118641 s
21/08/03 13:57:47 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:47 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:47 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:47 INFO SparkContext: Created broadcast 8 from textFile at ReadWrite.scala:587
21/08/03 13:57:47 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:47 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:47 INFO DAGScheduler: Got job 4 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:47 INFO DAGScheduler: Final stage: ResultStage 4 (first at ReadWrite.scala:587)
21/08/03 13:57:47 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:47 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:47 INFO DAGScheduler: Submitting ResultStage 4 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/metadata MapPartitionsRDD[9] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:47 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:47 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:47 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/metadata MapPartitionsRDD[9] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:47 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
21/08/03 13:57:47 INFO FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool 5302906234339048909
21/08/03 13:57:47 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.48.11.68, executor 9, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:47 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 21.8 KiB, free 3.3 GiB)
21/08/03 13:57:47 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 3.3 GiB)
21/08/03 13:57:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.48.11.69:43057 (size: 5.8 KiB, free: 3.3 GiB)
21/08/03 13:57:47 INFO SparkContext: Created broadcast 9 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.48.11.68:44227 (size: 5.8 KiB, free: 4.6 GiB)
21/08/03 13:57:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.48.11.68:44227 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 3017 ms on 10.48.11.68 (executor 9) (1/1)
21/08/03 13:57:50 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:50 INFO DAGScheduler: ResultStage 4 (first at ReadWrite.scala:587) finished in 3.021 s
21/08/03 13:57:50 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
21/08/03 13:57:50 INFO DAGScheduler: Job 4 finished: first at ReadWrite.scala:587, took 3.025845 s
21/08/03 13:57:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:50 INFO SparkContext: Created broadcast 10 from textFile at ReadWrite.scala:587
21/08/03 13:57:50 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:50 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:50 INFO DAGScheduler: Got job 5 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:50 INFO DAGScheduler: Final stage: ResultStage 5 (first at ReadWrite.scala:587)
21/08/03 13:57:50 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:50 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:50 INFO DAGScheduler: Submitting ResultStage 5 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/stages/0_vecAssembler_56d3c6a6e699/metadata MapPartitionsRDD[11] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:50 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:50 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:50 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/stages/0_vecAssembler_56d3c6a6e699/metadata MapPartitionsRDD[11] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:50 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
21/08/03 13:57:50 INFO FairSchedulableBuilder: Added task set TaskSet_5.0 tasks to pool 5302906234339048909
21/08/03 13:57:50 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.48.11.68, executor 9, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:50 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 21.9 KiB, free 3.3 GiB)
21/08/03 13:57:50 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.3 GiB)
21/08/03 13:57:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.48.11.69:43057 (size: 5.9 KiB, free: 3.3 GiB)
21/08/03 13:57:50 INFO SparkContext: Created broadcast 11 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:50 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.48.11.68:44227 (size: 5.9 KiB, free: 4.6 GiB)
21/08/03 13:57:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.48.11.68:44227 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:50 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 96 ms on 10.48.11.68 (executor 9) (1/1)
21/08/03 13:57:50 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:50 INFO DAGScheduler: ResultStage 5 (first at ReadWrite.scala:587) finished in 0.100 s
21/08/03 13:57:50 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
21/08/03 13:57:50 INFO DAGScheduler: Job 5 finished: first at ReadWrite.scala:587, took 0.106143 s
21/08/03 13:57:50 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:50 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:50 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:50 INFO SparkContext: Created broadcast 12 from textFile at ReadWrite.scala:587
21/08/03 13:57:50 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:50 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:50 INFO DAGScheduler: Got job 6 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:50 INFO DAGScheduler: Final stage: ResultStage 6 (first at ReadWrite.scala:587)
21/08/03 13:57:50 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:50 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:50 INFO DAGScheduler: Submitting ResultStage 6 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/stages/0_vecAssembler_56d3c6a6e699/metadata MapPartitionsRDD[13] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:50 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:50 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:50 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/stages/0_vecAssembler_56d3c6a6e699/metadata MapPartitionsRDD[13] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:50 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
21/08/03 13:57:50 INFO FairSchedulableBuilder: Added task set TaskSet_6.0 tasks to pool 5302906234339048909
21/08/03 13:57:50 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (10.48.11.72, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:50 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 21.9 KiB, free 3.3 GiB)
21/08/03 13:57:50 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.3 GiB)
21/08/03 13:57:50 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.48.11.69:43057 (size: 5.9 KiB, free: 3.3 GiB)
21/08/03 13:57:50 INFO SparkContext: Created broadcast 13 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:50 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.48.11.72:43787 (size: 5.9 KiB, free: 4.6 GiB)
21/08/03 13:57:51 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.48.11.72:43787 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2999 ms on 10.48.11.72 (executor 0) (1/1)
21/08/03 13:57:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:53 INFO DAGScheduler: ResultStage 6 (first at ReadWrite.scala:587) finished in 3.009 s
21/08/03 13:57:53 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
21/08/03 13:57:53 INFO DAGScheduler: Job 6 finished: first at ReadWrite.scala:587, took 3.012185 s
21/08/03 13:57:53 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:53 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:53 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:53 INFO SparkContext: Created broadcast 14 from textFile at ReadWrite.scala:587
21/08/03 13:57:53 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:53 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:53 INFO DAGScheduler: Got job 7 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:53 INFO DAGScheduler: Final stage: ResultStage 7 (first at ReadWrite.scala:587)
21/08/03 13:57:53 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:53 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:53 INFO DAGScheduler: Submitting ResultStage 7 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/stages/1_xgbr_512bd622d263/metadata MapPartitionsRDD[15] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:53 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:53 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:53 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/stages/1_xgbr_512bd622d263/metadata MapPartitionsRDD[15] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:53 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
21/08/03 13:57:53 INFO FairSchedulableBuilder: Added task set TaskSet_7.0 tasks to pool 5302906234339048909
21/08/03 13:57:53 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (10.48.11.72, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:53 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 21.9 KiB, free 3.3 GiB)
21/08/03 13:57:53 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.3 GiB)
21/08/03 13:57:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.48.11.69:43057 (size: 5.9 KiB, free: 3.3 GiB)
21/08/03 13:57:53 INFO SparkContext: Created broadcast 15 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.48.11.72:43787 (size: 5.9 KiB, free: 4.6 GiB)
21/08/03 13:57:53 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.48.11.72:43787 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:53 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 143 ms on 10.48.11.72 (executor 0) (1/1)
21/08/03 13:57:53 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:53 INFO DAGScheduler: ResultStage 7 (first at ReadWrite.scala:587) finished in 0.155 s
21/08/03 13:57:53 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
21/08/03 13:57:53 INFO DAGScheduler: Job 7 finished: first at ReadWrite.scala:587, took 0.157921 s
21/08/03 13:57:53 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:53 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:53 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:53 INFO SparkContext: Created broadcast 16 from textFile at ReadWrite.scala:587
21/08/03 13:57:53 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:53 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:53 INFO DAGScheduler: Got job 8 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:53 INFO DAGScheduler: Final stage: ResultStage 8 (first at ReadWrite.scala:587)
21/08/03 13:57:53 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:53 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:53 INFO DAGScheduler: Submitting ResultStage 8 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/stages/1_xgbr_512bd622d263/metadata MapPartitionsRDD[17] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:53 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:53 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:53 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/estimator/stages/1_xgbr_512bd622d263/metadata MapPartitionsRDD[17] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:53 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
21/08/03 13:57:53 INFO FairSchedulableBuilder: Added task set TaskSet_8.0 tasks to pool 5302906234339048909
21/08/03 13:57:53 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (10.48.11.71, executor 3, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:53 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 21.9 KiB, free 3.3 GiB)
21/08/03 13:57:53 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.3 GiB)
21/08/03 13:57:53 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.48.11.69:43057 (size: 5.9 KiB, free: 3.3 GiB)
21/08/03 13:57:53 INFO SparkContext: Created broadcast 17 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:53 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.48.11.71:46151 (size: 5.9 KiB, free: 4.6 GiB)
21/08/03 13:57:53 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.48.11.71:46151 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:54 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 115 ms on 10.48.11.71 (executor 3) (1/1)
21/08/03 13:57:54 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:54 INFO DAGScheduler: ResultStage 8 (first at ReadWrite.scala:587) finished in 0.119 s
21/08/03 13:57:54 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
21/08/03 13:57:54 INFO DAGScheduler: Job 8 finished: first at ReadWrite.scala:587, took 0.124267 s
21/08/03 13:57:54 INFO Instrumentation: [d7a15d7f] training finished
21/08/03 13:57:54 INFO Instrumentation: [1731336c] training finished
21/08/03 13:57:54 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:54 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:54 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:54 INFO SparkContext: Created broadcast 18 from textFile at ReadWrite.scala:587
21/08/03 13:57:54 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:54 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:54 INFO DAGScheduler: Got job 9 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:54 INFO DAGScheduler: Final stage: ResultStage 9 (first at ReadWrite.scala:587)
21/08/03 13:57:54 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:54 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:54 INFO DAGScheduler: Submitting ResultStage 9 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/metadata MapPartitionsRDD[19] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:54 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:54 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:54 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/metadata MapPartitionsRDD[19] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:54 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
21/08/03 13:57:54 INFO FairSchedulableBuilder: Added task set TaskSet_9.0 tasks to pool 5302906234339048909
21/08/03 13:57:54 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (10.48.11.71, executor 2, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:54 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 21.8 KiB, free 3.3 GiB)
21/08/03 13:57:54 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 3.3 GiB)
21/08/03 13:57:54 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.48.11.69:43057 (size: 5.8 KiB, free: 3.3 GiB)
21/08/03 13:57:54 INFO SparkContext: Created broadcast 19 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:54 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.48.11.71:45657 (size: 5.8 KiB, free: 4.6 GiB)
21/08/03 13:57:55 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.48.11.71:45657 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 2941 ms on 10.48.11.71 (executor 2) (1/1)
21/08/03 13:57:57 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:57 INFO DAGScheduler: ResultStage 9 (first at ReadWrite.scala:587) finished in 2.945 s
21/08/03 13:57:57 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
21/08/03 13:57:57 INFO DAGScheduler: Job 9 finished: first at ReadWrite.scala:587, took 2.950493 s
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 20 from textFile at ReadWrite.scala:587
21/08/03 13:57:57 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:57 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:57 INFO DAGScheduler: Got job 10 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:57 INFO DAGScheduler: Final stage: ResultStage 10 (first at ReadWrite.scala:587)
21/08/03 13:57:57 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:57 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:57 INFO DAGScheduler: Submitting ResultStage 10 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/metadata MapPartitionsRDD[21] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:57 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:57 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:57 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/metadata MapPartitionsRDD[21] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:57 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
21/08/03 13:57:57 INFO FairSchedulableBuilder: Added task set TaskSet_10.0 tasks to pool 5302906234339048909
21/08/03 13:57:57 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (10.48.11.72, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 21.8 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.48.11.69:43057 (size: 5.8 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 21 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.48.11.72:43787 (size: 5.8 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.48.11.72:43787 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 70 ms on 10.48.11.72 (executor 0) (1/1)
21/08/03 13:57:57 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:57 INFO DAGScheduler: ResultStage 10 (first at ReadWrite.scala:587) finished in 0.074 s
21/08/03 13:57:57 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
21/08/03 13:57:57 INFO DAGScheduler: Job 10 finished: first at ReadWrite.scala:587, took 0.078421 s
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 22 from textFile at ReadWrite.scala:587
21/08/03 13:57:57 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:57 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:57 INFO DAGScheduler: Got job 11 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:57 INFO DAGScheduler: Final stage: ResultStage 11 (first at ReadWrite.scala:587)
21/08/03 13:57:57 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:57 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:57 INFO DAGScheduler: Submitting ResultStage 11 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/stages/0_vecAssembler_56d3c6a6e699/metadata MapPartitionsRDD[23] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:57 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:57 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:57 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/stages/0_vecAssembler_56d3c6a6e699/metadata MapPartitionsRDD[23] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:57 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
21/08/03 13:57:57 INFO FairSchedulableBuilder: Added task set TaskSet_11.0 tasks to pool 5302906234339048909
21/08/03 13:57:57 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (10.48.11.72, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 21.9 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.48.11.69:43057 (size: 5.9 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 23 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.48.11.72:43787 (size: 5.9 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.48.11.72:43787 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 64 ms on 10.48.11.72 (executor 0) (1/1)
21/08/03 13:57:57 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:57 INFO DAGScheduler: ResultStage 11 (first at ReadWrite.scala:587) finished in 0.066 s
21/08/03 13:57:57 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
21/08/03 13:57:57 INFO DAGScheduler: Job 11 finished: first at ReadWrite.scala:587, took 0.070614 s
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 24 from textFile at ReadWrite.scala:587
21/08/03 13:57:57 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:57 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:57 INFO DAGScheduler: Got job 12 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:57 INFO DAGScheduler: Final stage: ResultStage 12 (first at ReadWrite.scala:587)
21/08/03 13:57:57 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:57 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:57 INFO DAGScheduler: Submitting ResultStage 12 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/stages/0_vecAssembler_56d3c6a6e699/metadata MapPartitionsRDD[25] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:57 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:57 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:57 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/stages/0_vecAssembler_56d3c6a6e699/metadata MapPartitionsRDD[25] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:57 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
21/08/03 13:57:57 INFO FairSchedulableBuilder: Added task set TaskSet_12.0 tasks to pool 5302906234339048909
21/08/03 13:57:57 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (10.48.11.72, executor 0, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 21.9 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.48.11.69:43057 (size: 5.9 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 25 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.48.11.72:43787 (size: 5.9 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.48.11.72:43787 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 62 ms on 10.48.11.72 (executor 0) (1/1)
21/08/03 13:57:57 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:57 INFO DAGScheduler: ResultStage 12 (first at ReadWrite.scala:587) finished in 0.065 s
21/08/03 13:57:57 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
21/08/03 13:57:57 INFO DAGScheduler: Job 12 finished: first at ReadWrite.scala:587, took 0.069930 s
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 26 from textFile at ReadWrite.scala:587
21/08/03 13:57:57 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:57 INFO SparkContext: Starting job: first at ReadWrite.scala:587
21/08/03 13:57:57 INFO DAGScheduler: Got job 13 (first at ReadWrite.scala:587) with 1 output partitions
21/08/03 13:57:57 INFO DAGScheduler: Final stage: ResultStage 13 (first at ReadWrite.scala:587)
21/08/03 13:57:57 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:57 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:57 INFO DAGScheduler: Submitting ResultStage 13 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/stages/1_xgbr_512bd622d263/metadata MapPartitionsRDD[27] at textFile at ReadWrite.scala:587), which has no missing parents
21/08/03 13:57:57 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:57 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:57 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/stages/1_xgbr_512bd622d263/metadata MapPartitionsRDD[27] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:57 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
21/08/03 13:57:57 INFO FairSchedulableBuilder: Added task set TaskSet_13.0 tasks to pool 5302906234339048909
21/08/03 13:57:57 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (10.48.11.68, executor 9, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 21.9 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.48.11.69:43057 (size: 5.9 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 27 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.48.11.68:44227 (size: 5.9 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.48.11.68:44227 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 99 ms on 10.48.11.68 (executor 9) (1/1)
21/08/03 13:57:57 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:57 INFO DAGScheduler: ResultStage 13 (first at ReadWrite.scala:587) finished in 0.104 s
21/08/03 13:57:57 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
21/08/03 13:57:57 INFO DAGScheduler: Job 13 finished: first at ReadWrite.scala:587, took 0.106547 s
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 40.0 B, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.48.11.69:43057 (size: 27.0 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 28 from textFile at DefaultXGBoostParamsReader.scala:82
21/08/03 13:57:57 INFO FileInputFormat: Total input paths to process : 1
21/08/03 13:57:57 INFO SparkContext: Starting job: first at DefaultXGBoostParamsReader.scala:82
21/08/03 13:57:57 INFO DAGScheduler: Got job 14 (first at DefaultXGBoostParamsReader.scala:82) with 1 output partitions
21/08/03 13:57:57 INFO DAGScheduler: Final stage: ResultStage 14 (first at DefaultXGBoostParamsReader.scala:82)
21/08/03 13:57:57 INFO DAGScheduler: Parents of final stage: List()
21/08/03 13:57:57 INFO DAGScheduler: Missing parents: List()
21/08/03 13:57:57 INFO DAGScheduler: Submitting ResultStage 14 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/stages/1_xgbr_512bd622d263/metadata MapPartitionsRDD[29] at textFile at DefaultXGBoostParamsReader.scala:82), which has no missing parents
21/08/03 13:57:57 INFO DAGScheduler: Jars for session None: Map(spark://10.48.11.69:38103/jars/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730735, spark://10.48.11.69:38103/jars/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731597, spark://10.48.11.69:38103/jars/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730537, spark://10.48.11.69:38103/jars/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731521, spark://10.48.11.69:38103/jars/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730232, spark://10.48.11.69:38103/jars/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729872, spark://10.48.11.69:38103/jars/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730074, spark://10.48.11.69:38103/jars/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731249, spark://10.48.11.69:38103/jars/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731683, spark://10.48.11.69:38103/jars/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729744, spark://10.48.11.69:38103/jars/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731406, spark://10.48.11.69:38103/jars/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731153, spark://10.48.11.69:38103/jars/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731896, spark://10.48.11.69:38103/jars/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730805)
21/08/03 13:57:57 INFO DAGScheduler: Files for session None: Map(spark://10.48.11.69:38103/files/addedFile3709140625277650444xgboost4j_2_12_1_4_1-65867.jar -> 1627998730522, spark://10.48.11.69:38103/files/addedFile1254174918966980822scalatest_2_12_3_0_5-e01bc.jar -> 1627998731135, spark://10.48.11.69:38103/files/addedFile5291058496637382086xgboost4j_spark_2_12_1_4_1-c3887.jar -> 1627998731672, spark://10.48.11.69:38103/files/addedFile8305477370858712955objenesis_2_5_1-7628f.jar -> 1627998729829, spark://10.48.11.69:38103/files/addedFile3247248354932916614reflectasm_1_11_3-d1a74.jar -> 1627998729694, spark://10.48.11.69:38103/files/addedFile1775982106135659717config_1_3_3-49c52.jar -> 1627998730036, spark://10.48.11.69:38103/files/addedFile1526939427740944743kryo_4_0_2-97606.jar -> 1627998731785, spark://10.48.11.69:38103/files/addedFile3160972677795486384scala_xml_2_12_1_0_6-6140a.jar -> 1627998730219, spark://10.48.11.69:38103/files/addedFile5380999882069665220scalactic_2_12_3_0_5-d3e58.jar -> 1627998731503, spark://10.48.11.69:38103/files/addedFile1052673451178424914asm_5_0_4-f9eec.jar -> 1627998731212, spark://10.48.11.69:38103/files/addedFile3567578410798111494akka_actor_2_12_2_5_23-a413b.jar -> 1627998730721, spark://10.48.11.69:38103/files/addedFile6675069858488438437minlog_1_3_0-88421.jar -> 1627998731576, spark://10.48.11.69:38103/files/addedFile8241248691510676812commons_logging_1_2-0b642.jar -> 1627998730782, spark://10.48.11.69:38103/files/addedFile8437665492112908622scala_java8_compat_2_12_0_8_0-0a56b.jar -> 1627998731393)
21/08/03 13:57:57 INFO DAGScheduler: Archives for session None: Map()
21/08/03 13:57:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (/user/akshay/NL_RM/data/spark-train-validation-split-28072021/bestModel/stages/1_xgbr_512bd622d263/metadata MapPartitionsRDD[29] at textFile at DefaultXGBoostParamsReader.scala:82) (first 15 tasks are for partitions Vector(0))
21/08/03 13:57:57 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
21/08/03 13:57:57 INFO FairSchedulableBuilder: Added task set TaskSet_14.0 tasks to pool 5302906234339048909
21/08/03 13:57:57 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (10.48.11.72, executor 1, partition 0, PROCESS_LOCAL, taskResourceAssignments Map())
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 21.9 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 3.3 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.48.11.69:43057 (size: 5.8 KiB, free: 3.3 GiB)
21/08/03 13:57:57 INFO SparkContext: Created broadcast 29 from broadcast at TaskSetManager.scala:552
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.48.11.72:45133 (size: 5.8 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.48.11.72:45133 (size: 27.0 KiB, free: 4.6 GiB)
21/08/03 13:57:57 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 135 ms on 10.48.11.72 (executor 1) (1/1)
21/08/03 13:57:57 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 5302906234339048909
21/08/03 13:57:57 INFO DAGScheduler: ResultStage 14 (first at DefaultXGBoostParamsReader.scala:82) finished in 0.138 s
21/08/03 13:57:57 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
21/08/03 13:57:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
21/08/03 13:57:57 INFO DAGScheduler: Job 14 finished: first at DefaultXGBoostParamsReader.scala:82, took 0.142503 s
21/08/03 13:57:58 ERROR Instrumentation: ml.dmlc.xgboost4j.java.XGBoostError: std::bad_alloc
	at ml.dmlc.xgboost4j.java.XGBoostJNI.checkCall(XGBoostJNI.java:48)
	at ml.dmlc.xgboost4j.java.Booster.loadModel(Booster.java:81)
	at ml.dmlc.xgboost4j.java.XGBoost.loadModel(XGBoost.java:64)
	at ml.dmlc.xgboost4j.scala.XGBoost$.loadModel(XGBoost.scala:157)
	at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressionModel$XGBoostRegressionModelReader.load(XGBoostRegressor.scala:478)
	at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressionModel$XGBoostRegressionModelReader.load(XGBoostRegressor.scala:464)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)
	at org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)
	at org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)
	at org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstance(ReadWrite.scala:622)
	at org.apache.spark.ml.tuning.TrainValidationSplitModel$TrainValidationSplitModelReader.load(TrainValidationSplit.scala:399)
	at org.apache.spark.ml.tuning.TrainValidationSplitModel$TrainValidationSplitModelReader.load(TrainValidationSplit.scala:388)
	at org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:355)
	at org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:355)
	at org.apache.spark.ml.tuning.TrainValidationSplitModel$.load(TrainValidationSplit.scala:341)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-3736905949128667:2)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-3736905949128667:43)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw$$iw.<init>(command-3736905949128667:45)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw.<init>(command-3736905949128667:47)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw.<init>(command-3736905949128667:49)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw.<init>(command-3736905949128667:51)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read.<init>(command-3736905949128667:53)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$.<init>(command-3736905949128667:57)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$.<clinit>(command-3736905949128667)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$eval$.$print$lzycompute(<notebook>:7)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$eval$.$print(<notebook>:6)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:235)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:903)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:856)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:235)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$13(DriverLocal.scala:544)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:53)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:53)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:521)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:689)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:681)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:522)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:634)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:427)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:370)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:221)
	at java.lang.Thread.run(Thread.java:748)

21/08/03 13:57:58 ERROR Instrumentation: ml.dmlc.xgboost4j.java.XGBoostError: std::bad_alloc
	at ml.dmlc.xgboost4j.java.XGBoostJNI.checkCall(XGBoostJNI.java:48)
	at ml.dmlc.xgboost4j.java.Booster.loadModel(Booster.java:81)
	at ml.dmlc.xgboost4j.java.XGBoost.loadModel(XGBoost.java:64)
	at ml.dmlc.xgboost4j.scala.XGBoost$.loadModel(XGBoost.scala:157)
	at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressionModel$XGBoostRegressionModelReader.load(XGBoostRegressor.scala:478)
	at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressionModel$XGBoostRegressionModelReader.load(XGBoostRegressor.scala:464)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)
	at org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)
	at org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)
	at org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstance(ReadWrite.scala:622)
	at org.apache.spark.ml.tuning.TrainValidationSplitModel$TrainValidationSplitModelReader.load(TrainValidationSplit.scala:399)
	at org.apache.spark.ml.tuning.TrainValidationSplitModel$TrainValidationSplitModelReader.load(TrainValidationSplit.scala:388)
	at org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:355)
	at org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:355)
	at org.apache.spark.ml.tuning.TrainValidationSplitModel$.load(TrainValidationSplit.scala:341)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-3736905949128667:2)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-3736905949128667:43)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw$$iw.<init>(command-3736905949128667:45)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw.<init>(command-3736905949128667:47)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw.<init>(command-3736905949128667:49)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw.<init>(command-3736905949128667:51)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read.<init>(command-3736905949128667:53)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$.<init>(command-3736905949128667:57)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$.<clinit>(command-3736905949128667)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$eval$.$print$lzycompute(<notebook>:7)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$eval$.$print(<notebook>:6)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:235)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:903)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:856)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:235)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$13(DriverLocal.scala:544)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:53)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:53)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:521)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:689)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:681)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:522)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:634)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:427)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:370)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:221)
	at java.lang.Thread.run(Thread.java:748)

21/08/03 13:57:58 ERROR ScalaDriverLocal: User Code Stack Trace: 
ml.dmlc.xgboost4j.java.XGBoostError: std::bad_alloc
	at ml.dmlc.xgboost4j.java.XGBoostJNI.checkCall(XGBoostJNI.java:48)
	at ml.dmlc.xgboost4j.java.Booster.loadModel(Booster.java:81)
	at ml.dmlc.xgboost4j.java.XGBoost.loadModel(XGBoost.java:64)
	at ml.dmlc.xgboost4j.scala.XGBoost$.loadModel(XGBoost.scala:157)
	at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressionModel$XGBoostRegressionModelReader.load(XGBoostRegressor.scala:478)
	at ml.dmlc.xgboost4j.scala.spark.XGBoostRegressionModel$XGBoostRegressionModelReader.load(XGBoostRegressor.scala:464)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)
	at org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)
	at org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)
	at org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)
	at org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)
	at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)
	at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349)
	at org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstance(ReadWrite.scala:622)
	at org.apache.spark.ml.tuning.TrainValidationSplitModel$TrainValidationSplitModelReader.load(TrainValidationSplit.scala:399)
	at org.apache.spark.ml.tuning.TrainValidationSplitModel$TrainValidationSplitModelReader.load(TrainValidationSplit.scala:388)
	at org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:355)
	at org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:355)
	at org.apache.spark.ml.tuning.TrainValidationSplitModel$.load(TrainValidationSplit.scala:341)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-3736905949128667:2)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-3736905949128667:43)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw$$iw.<init>(command-3736905949128667:45)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw$$iw.<init>(command-3736905949128667:47)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw$$iw.<init>(command-3736905949128667:49)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$$iw.<init>(command-3736905949128667:51)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read.<init>(command-3736905949128667:53)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$.<init>(command-3736905949128667:57)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$read$.<clinit>(command-3736905949128667)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$eval$.$print$lzycompute(<notebook>:7)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$eval$.$print(<notebook>:6)
	at $line26b7a5959a12454694f9bfa15f0ab9a725.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:235)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:903)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:856)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:235)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$13(DriverLocal.scala:544)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:53)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:53)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:521)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:689)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:681)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:522)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:634)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:427)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:370)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:221)
	at java.lang.Thread.run(Thread.java:748)
21/08/03 13:57:58 INFO ProgressReporter$: Removed result fetcher for 5302906234339048909_8057104574846571832_fb011d46071d49fb85b417f33f4e599b
